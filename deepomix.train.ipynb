{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c54281-3ee8-4a9b-8961-5714c5406697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9ea01a-9fc6-4bf3-809d-472d8a37ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for deppomix train\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "deg_df = pd.read_csv('./ana_20240318/tmp_deg_fc_surv_df.xls', sep='\\t')\n",
    "dmp_df = pd.read_csv('./ana_20221121/tmp_dmg_delta_beta_surv_df.xls', sep='\\t')\n",
    "mut_df = pd.read_csv('./ana_20221121/mut_gene_surv_df.xls', sep='\\t')\n",
    "\n",
    "deg_coef = pd.read_csv('./ana_20221116/lasso.feature.selection.RNA.deg.res.xls', sep='\\t')\n",
    "dmp_coef = pd.read_csv('./ana_20221116/lasso.feature.selection.RNA.dmg.res.xls', sep='\\t')\n",
    "mut_coef = pd.read_csv('./ana_20221116/lasso.feature.selection.mutation.res.xls', sep='\\t')\n",
    "# deg_df.columns\n",
    "\n",
    "\n",
    "# mut_select_df = mut_df.loc[:, [\"Tumor_Sample_Barcode\", \"OS\", \"survival\"] + list(mut_coef.index.values)]\n",
    "\n",
    "# deg_df.columns = [\"Tumor_Sample_Barcode\", \"OS\", \"survival\", ,\"OS.time\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a87983-c82f-4e47-963e-4e7b0299ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_select_df = mut_df.loc[:, [\"Tumor_Sample_Barcode\", \"OS\", \"survival\"] + list(mut_coef.index.values)]\n",
    "deg_l = [\"patient\", \"OS\", \"survival\"] + list(deg_coef.gene_id.values)\n",
    "deg_l[11]\n",
    "deg_l[11] = 'ENSG00000171540.8'\n",
    "deg_l[11]\n",
    "deg_select_df = deg_df.loc[:, deg_l]\n",
    "dmp_select_df = dmp_df.loc[:, [\"patient\", \"OS\", \"survival\"] + list(dmp_coef.index.values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5194d078-4c99-403d-90e2-338ba6da4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "511\n",
      "512\n",
      "501\n",
      "511\n",
      "512\n",
      "498\n",
      "(498, 9)\n",
      "(498, 25)\n",
      "(498, 14)\n",
      "Index(['Tumor_Sample_Barcode', 'OS', 'survival', 'IDH2', 'CIC', 'IDH1',\n",
      "       'MUC16', 'EGFR', 'SMARCA4'],\n",
      "      dtype='object')\n",
      "Index(['patient', 'OS', 'survival', 'ENSG00000169594.13', 'ENSG00000229108.2',\n",
      "       'ENSG00000198353.8', 'ENSG00000197903.8', 'ENSG00000073792.16',\n",
      "       'ENSG00000108846.16', 'ENSG00000215612.8', 'ENSG00000187546.14',\n",
      "       'ENSG00000171540.8', 'ENSG00000164309.15', 'ENSG00000166483.11',\n",
      "       'ENSG00000121211.8', 'ENSG00000275830.2', 'ENSG00000164761.9',\n",
      "       'ENSG00000168447.11', 'ENSG00000184895.8', 'ENSG00000176842.15',\n",
      "       'ENSG00000122592.8', 'ENSG00000256637.7', 'ENSG00000159556.10',\n",
      "       'ENSG00000142700.12', 'ENSG00000115457.10'],\n",
      "      dtype='object')\n",
      "Index(['patient', 'OS', 'survival', 'KIAA1598', 'TUBA1B', 'WEE1', 'PARK2',\n",
      "       'CRYGD', 'PDGFB', 'E2F2', 'GLT25D2', 'BDNFOS', 'DHX36', 'SRRM4'],\n",
      "      dtype='object')\n",
      "Index(['patient', 'OS', 'survival', 'IDH2', 'CIC', 'IDH1', 'MUC16', 'EGFR',\n",
      "       'SMARCA4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# merge table\n",
    "mut_sample = list(mut_select_df.Tumor_Sample_Barcode.values)\n",
    "deg_sample = list(deg_select_df.patient.values)\n",
    "dmp_sample = list(dmp_select_df.patient.values)\n",
    "\n",
    "print(len(mut_sample))\n",
    "print(len(deg_sample))\n",
    "print(len(dmp_sample))\n",
    "\n",
    "print(len(set(mut_sample)))\n",
    "print(len(set(deg_sample)))\n",
    "print(len(set(dmp_sample)))\n",
    "\n",
    "tmp = set(mut_sample).intersection(set(deg_sample))\n",
    "tmp = tmp.intersection(set(dmp_sample))\n",
    "print(len(tmp))\n",
    "\n",
    "mut_index = [i in tmp for i in mut_sample]\n",
    "mut_index_1 = mut_select_df.index.values[mut_index]\n",
    "mut_select_df1 = mut_select_df.iloc[mut_index_1, :]\n",
    "print(mut_select_df1.shape)\n",
    "\n",
    "mut_index = [i in tmp for i in deg_sample]\n",
    "mut_index_1 = deg_select_df.index.values[mut_index]\n",
    "deg_select_df1 = deg_select_df.iloc[mut_index_1, :]\n",
    "print(deg_select_df1.shape)\n",
    "\n",
    "mut_index = [i in tmp for i in dmp_sample]\n",
    "mut_index_1 = dmp_select_df.index.values[mut_index]\n",
    "dmp_select_df1 = dmp_select_df.iloc[mut_index_1, :]\n",
    "print(dmp_select_df1.shape)\n",
    "\n",
    "\n",
    "print(mut_select_df1.columns)\n",
    "print(deg_select_df1.columns)\n",
    "print(dmp_select_df1.columns)\n",
    "sum(mut_select_df1.Tumor_Sample_Barcode.values == deg_select_df1.patient.values)\n",
    "sum(mut_select_df1.Tumor_Sample_Barcode.values == dmp_select_df1.patient.values)\n",
    "sum(dmp_select_df1.patient.values == deg_select_df1.patient.values)\n",
    "\n",
    "tmp1 = mut_select_df1.columns.values\n",
    "tmp1[0] = 'patient'\n",
    "mut_select_df1.columns = tmp1\n",
    "print(mut_select_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a0d5e7-ee28-4f06-a6c8-6939b7bc93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge table\n",
    "x_columns = ['IDH2', 'CIC', 'IDH1', 'MUC16', 'EGFR',\n",
    "       'SMARCA4', 'ENSG00000169594.13', 'ENSG00000229108.2',\n",
    "       'ENSG00000198353.8', 'ENSG00000197903.8', 'ENSG00000073792.16',\n",
    "       'ENSG00000108846.16', 'ENSG00000215612.8', 'ENSG00000187546.14',\n",
    "       'ENSG00000171540.8', 'ENSG00000164309.15', 'ENSG00000166483.11',\n",
    "       'ENSG00000121211.8', 'ENSG00000275830.2', 'ENSG00000164761.9',\n",
    "       'ENSG00000168447.11', 'ENSG00000184895.8', 'ENSG00000176842.15',\n",
    "       'ENSG00000122592.8', 'ENSG00000256637.7', 'ENSG00000159556.10',\n",
    "       'ENSG00000142700.12', 'ENSG00000115457.10', 'KIAA1598', 'TUBA1B', 'WEE1', 'PARK2',\n",
    "       'CRYGD', 'PDGFB', 'E2F2', 'GLT25D2', 'BDNFOS', 'DHX36', 'SRRM4']\n",
    "\n",
    "y_time_column = 'OS'\n",
    "y_event_column = 'survival'\n",
    "\n",
    "merge_df = pd.merge(mut_select_df1, deg_select_df1, on='patient')\n",
    "merge_df = pd.merge(merge_df, dmp_select_df1, on = 'patient')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ee7a57-3f93-47ff-8030-a5e7f4523237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "# random select samples \n",
    "import random\n",
    "random.seed(133)\n",
    "# select 70% samples\n",
    "print(int(merge_df.shape[0] * 0.7) + int(merge_df.shape[0] * 0.3))\n",
    "merge_df.shape[0]\n",
    "train_index = random.sample(list(merge_df.index.values), int(merge_df.shape[0] * 0.7) )\n",
    "# test_index_i = [i not in train_index for i in list(merge_df.index.values)]\n",
    "# test_index = list(merge_df.index.values)[test_index_i]\n",
    "test_index = []\n",
    "for i in list(merge_df.index.values):\n",
    "    if i not in train_index:\n",
    "        test_index.append(i)\n",
    "\n",
    "tmp2 = train_index + test_index \n",
    "print(sum([ i in tmp2 for i in  merge_df.index.values]))\n",
    "train_df = merge_df.iloc[train_index, :]\n",
    "test_df = merge_df.iloc[test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd7365f-bd9f-4d37-9c39-ec7113a623ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X_train = train_df.loc[:,x_columns]\n",
    "y_time_train = train_df.loc[:,['OS']]\n",
    "y_event_train = train_df.loc[:,['survival']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e02228d-2cbd-4c56-b5ca-654f04003777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.values\n",
    "# y_event_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e1985c-e968-40c2-b37a-d79b95442e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepOmixNet(nn.Module):\n",
    "\tdef __init__(self, In_Nodes, Pathway_Nodes, Hidden_Nodes,Out_Nodes, Pathway_Mask):\n",
    "\t\tsuper(DeepOmixNet, self).__init__()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.pathway_mask = Pathway_Mask\n",
    "\t\tself.sc1 = nn.Linear(In_Nodes, Pathway_Nodes)\n",
    "\t\tself.sc2 = nn.Linear(Pathway_Nodes, Hidden_Nodes)\n",
    "\t\t# self.sc2 = nn.Linear(In_Nodes, Hidden_Nodes)\n",
    "\t\tself.sc3 = nn.Linear(Hidden_Nodes, Out_Nodes, bias=False)\n",
    "\t\tself.sc4 = nn.Linear(Out_Nodes+1,1, bias = False)\n",
    "\t\tself.sc4.weight.data.uniform_(-0.001, 0.001)\n",
    "\t\tself.do_m1 = torch.ones(Pathway_Nodes)\n",
    "\t\tself.do_m2 = torch.ones(Hidden_Nodes)\n",
    "\t\t###if gpu is being used\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tself.do_m1 = self.do_m1.cuda()\n",
    "\t\t\tself.do_m2 = self.do_m2.cuda()\n",
    "\n",
    "\tdef forward(self, x_1, x_2):\n",
    "\t\tself.sc1.weight.data = self.sc1.weight.data.mul(self.pathway_mask)\n",
    "\t\tx_1 = self.tanh(self.sc1(x_1))\n",
    "\t\tif self.training == True: \n",
    "\t\t\tx_1 = x_1.mul(self.do_m1)\n",
    "\t\tx_1 = self.tanh(self.sc2(x_1))\n",
    "\t\tif self.training == True: \n",
    "\t\t\tx_1 = x_1.mul(self.do_m2)\n",
    "\t\tx_1 = self.tanh(self.sc3(x_1))\n",
    "\t\tx_cat = torch.cat((x_1, x_2), 1)\n",
    "\t\tlin_pred = self.sc4(x_cat)\n",
    "\t\t\n",
    "\t\treturn lin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a27e6ec-e4b3-4db9-a6e0-f71478870e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# from DeepOmix import DeepOmixNet\n",
    "from SubNetwork import dropout_mask, s_mask\n",
    "from Survival import R_set, neg_par_log_likelihood, c_index\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "gpu_id=0\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "\ttorch.cuda.set_device(gpu_id)\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "print(device)\n",
    "def trainDeepOmixNet_without(train_x, train_ytime, train_yevent, \\\n",
    "\t\t\teval_x, eval_ytime, eval_yevent, pathway_mask, \\\n",
    "\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\tLearning_Rate, L2, Num_Epochs, Dropout_Rate):\n",
    "\tprint('there are ',Num_Epochs,'epochs in the training process!!')\n",
    "\tnet = DeepOmixNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)\n",
    "\t###if gpu is being used\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tnet.cuda()\n",
    "\t# net.to(device)\n",
    "\t# tensor.to(device)\n",
    "\topt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "\tfor epoch in range(Num_Epochs+1):\n",
    "\t\tnet.train()\n",
    "\t\topt.zero_grad() \n",
    "\t\tnet.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])\n",
    "\t\tnet.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])\n",
    "\t\t# print(net)\n",
    "\t\t# print(net)\n",
    "\n",
    "\t\t# train_x.to(device)\n",
    "\t\t# train_yevent.to(device)\n",
    "\t\t# train_yevent.to(device)\n",
    "\t\t# train_x.to(device)\n",
    "\t\tpred = net(train_x, train_yevent) \n",
    "\t\tloss = neg_par_log_likelihood(pred, train_ytime, train_yevent) \n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\tnet.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask)\n",
    "\n",
    "\t\tdo_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)\n",
    "\t\tdo_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)\n",
    "\t\tdo_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))\n",
    "\t\tdo_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))\n",
    "\n",
    "\t\tnet_sc2_weight = copy.deepcopy(net.sc2.weight.data)\n",
    "\t\tnet_sc3_weight = copy.deepcopy(net.sc3.weight.data)\n",
    "\n",
    "\t\tnet_state_dict = net.state_dict()\n",
    "\n",
    "\t\tcopy_net = copy.deepcopy(net)\n",
    "\t\tcopy_state_dict = copy_net.state_dict()\n",
    "\t\tfor name, param in copy_state_dict.items():\n",
    "\n",
    "\t\t\tif not \"weight\" in name:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif \"sc1\" in name:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif \"sc4\" in name:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif \"sc2\" in name:\n",
    "\t\t\t\tactive_param = net_sc2_weight.mul(do_m1_grad_mask)\n",
    "\t\t\tif \"sc3\" in name:\n",
    "\t\t\t\tactive_param = net_sc3_weight.mul(do_m2_grad_mask)\n",
    "\t\t\tnonzero_param_1d = active_param[active_param != 0]\n",
    "\t\t\tif nonzero_param_1d.size(0) == 0: \n",
    "\t\t\t\tbreak\n",
    "\t\t\tcopy_param_1d = copy.deepcopy(nonzero_param_1d)\n",
    "\t\t\tS_set =  torch.arange(100, -1, -1)[1:]\n",
    "\t\t\tcopy_param = copy.deepcopy(active_param)\n",
    "\t\t\tS_loss = []\n",
    "\t\t\tfor S in S_set:\n",
    "\t\t\t\tparam_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "\t\t\t\ttransformed_param = copy_param.mul(param_mask)\n",
    "\t\t\t\tcopy_state_dict[name].copy_(transformed_param)\n",
    "\t\t\t\tcopy_net.train()\n",
    "\t\t\t\ty_tmp = copy_net(train_x, train_yevent)\n",
    "\t\t\t\tloss_tmp = neg_par_log_likelihood(y_tmp, train_ytime, train_yevent)\n",
    "\t\t\t\t# S_loss.append(loss_tmp.cpu())\n",
    "\t\t\t\tS_loss.append(loss_tmp.detach().cpu())\n",
    "\t\t\t\t# S_loss.append(loss_tmp.detach())\n",
    "\t\t\t# interp_S_loss = interp1d(S_set, S_loss, kind='cubic')\n",
    "\t\t\t# interp_S_loss = interp1d(S_set, S_loss, kind='cubic')\n",
    "\t\t\tinterp_S_loss = interp1d(S_set, torch.cat(S_loss), kind='cubic')\n",
    "\t\t\tinterp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)\n",
    "\t\t\tinterp_loss = interp_S_loss(interp_S_set)\n",
    "\t\t\toptimal_S = interp_S_set[np.argmin(interp_loss)]\n",
    "\t\t\toptimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "\t\t\tif \"sc2\" in name:\n",
    "\t\t\t\tfinal_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)\n",
    "\t\t\t\toptimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)\n",
    "\t\t\tif \"sc3\" in name:\n",
    "\t\t\t\tfinal_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)\n",
    "\t\t\t\toptimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)\n",
    "\t\t\tcopy_state_dict[name].copy_(optimal_transformed_param)\n",
    "\t\t\tnet_state_dict[name].copy_(optimal_transformed_param)\n",
    "\n",
    "\t\tif epoch % 20 == 0: \n",
    "\t\t\tnet.train()\n",
    "\t\t\ttrain_pred = net(train_x, train_yevent)\n",
    "\t\t\ttrain_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent).view(1,).item()\n",
    "\n",
    "\t\t\tnet.eval()\n",
    "\t\t\teval_pred = net(eval_x, eval_yevent)\n",
    "\t\t\teval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent).view(1,).item()\n",
    "\n",
    "\t\t\ttrain_cindex = c_index(train_pred, train_ytime, train_yevent)\n",
    "\t\t\teval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)\n",
    "\t\t\tprint(\"The \",epoch,\"th epoch : Loss in Train: \", train_loss)\n",
    "\t\t\tif (math.isnan(train_loss)):\n",
    "\t\t\t\tprint(epoch,train_loss,\"end_train\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn (train_loss, eval_loss, train_cindex, eval_cindex, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc47fe39-8e8c-401b-988b-65cc6c2acf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.695775032043457\n",
      "The  20 th epoch : Loss in Train:  4.652929306030273\n",
      "The  40 th epoch : Loss in Train:  4.611469268798828\n",
      "The  60 th epoch : Loss in Train:  4.570202350616455\n",
      "The  80 th epoch : Loss in Train:  4.530178070068359\n",
      "The  100 th epoch : Loss in Train:  4.491418838500977\n",
      "The  120 th epoch : Loss in Train:  4.45392370223999\n",
      "The  140 th epoch : Loss in Train:  4.4176859855651855\n",
      "The  160 th epoch : Loss in Train:  4.382698059082031\n",
      "The  180 th epoch : Loss in Train:  4.348947048187256\n",
      "The  200 th epoch : Loss in Train:  4.316417694091797\n",
      "The  220 th epoch : Loss in Train:  4.285094261169434\n",
      "The  240 th epoch : Loss in Train:  4.254957675933838\n",
      "The  260 th epoch : Loss in Train:  4.225984573364258\n",
      "The  280 th epoch : Loss in Train:  4.1981520652771\n",
      "The  300 th epoch : Loss in Train:  4.171433925628662\n",
      "The  320 th epoch : Loss in Train:  4.1458048820495605\n",
      "The  340 th epoch : Loss in Train:  4.121235370635986\n",
      "The  360 th epoch : Loss in Train:  4.097695827484131\n",
      "The  380 th epoch : Loss in Train:  4.075155735015869\n",
      "The  400 th epoch : Loss in Train:  4.053585529327393\n",
      "The  420 th epoch : Loss in Train:  4.032952785491943\n",
      "The  440 th epoch : Loss in Train:  4.013225555419922\n",
      "The  460 th epoch : Loss in Train:  3.9943735599517822\n",
      "The  480 th epoch : Loss in Train:  3.9763641357421875\n",
      "The  500 th epoch : Loss in Train:  3.9591658115386963\n",
      "The  520 th epoch : Loss in Train:  3.9427483081817627\n",
      "The  540 th epoch : Loss in Train:  3.9270801544189453\n",
      "The  560 th epoch : Loss in Train:  3.9121322631835938\n",
      "The  580 th epoch : Loss in Train:  3.897874593734741\n",
      "The  600 th epoch : Loss in Train:  3.8842785358428955\n",
      "The  620 th epoch : Loss in Train:  3.871316909790039\n",
      "The  640 th epoch : Loss in Train:  3.858961343765259\n",
      "The  660 th epoch : Loss in Train:  3.8471872806549072\n",
      "The  680 th epoch : Loss in Train:  3.835968017578125\n",
      "The  700 th epoch : Loss in Train:  3.8252792358398438\n",
      "The  720 th epoch : Loss in Train:  3.815098524093628\n",
      "The  740 th epoch : Loss in Train:  3.805401563644409\n",
      "The  760 th epoch : Loss in Train:  3.7961671352386475\n",
      "The  780 th epoch : Loss in Train:  3.7873756885528564\n",
      "The  800 th epoch : Loss in Train:  3.779005289077759\n",
      "The  820 th epoch : Loss in Train:  3.7710373401641846\n",
      "The  840 th epoch : Loss in Train:  3.763453722000122\n",
      "The  860 th epoch : Loss in Train:  3.7562367916107178\n",
      "The  880 th epoch : Loss in Train:  3.7493698596954346\n",
      "The  900 th epoch : Loss in Train:  3.7428364753723145\n",
      "The  920 th epoch : Loss in Train:  3.736621618270874\n",
      "The  940 th epoch : Loss in Train:  3.730710506439209\n",
      "The  960 th epoch : Loss in Train:  3.7250895500183105\n",
      "The  980 th epoch : Loss in Train:  3.719744920730591\n",
      "The  1000 th epoch : Loss in Train:  3.714665174484253\n",
      "The  1020 th epoch : Loss in Train:  3.709836006164551\n",
      "The  1040 th epoch : Loss in Train:  3.7052478790283203\n",
      "The  1060 th epoch : Loss in Train:  3.7008888721466064\n",
      "The  1080 th epoch : Loss in Train:  3.696748733520508\n",
      "The  1100 th epoch : Loss in Train:  3.692816972732544\n",
      "The  1120 th epoch : Loss in Train:  3.689084529876709\n",
      "The  1140 th epoch : Loss in Train:  3.685542345046997\n",
      "The  1160 th epoch : Loss in Train:  3.6821818351745605\n",
      "The  1180 th epoch : Loss in Train:  3.6789939403533936\n",
      "The  1200 th epoch : Loss in Train:  3.675971508026123\n",
      "The  1220 th epoch : Loss in Train:  3.6731069087982178\n",
      "The  1240 th epoch : Loss in Train:  3.6703925132751465\n",
      "The  1260 th epoch : Loss in Train:  3.6678214073181152\n",
      "The  1280 th epoch : Loss in Train:  3.6653876304626465\n",
      "The  1300 th epoch : Loss in Train:  3.663084030151367\n",
      "The  1320 th epoch : Loss in Train:  3.6609046459198\n",
      "The  1340 th epoch : Loss in Train:  3.658844470977783\n",
      "The  1360 th epoch : Loss in Train:  3.6568973064422607\n",
      "The  1380 th epoch : Loss in Train:  3.655057430267334\n",
      "The  1400 th epoch : Loss in Train:  3.6533210277557373\n",
      "The  1420 th epoch : Loss in Train:  3.651681900024414\n",
      "The  1440 th epoch : Loss in Train:  3.6501359939575195\n",
      "The  1460 th epoch : Loss in Train:  3.648679256439209\n",
      "The  1480 th epoch : Loss in Train:  3.6473066806793213\n",
      "The  1500 th epoch : Loss in Train:  3.646014928817749\n",
      "The  1520 th epoch : Loss in Train:  3.6447994709014893\n",
      "The  1540 th epoch : Loss in Train:  3.6436562538146973\n",
      "The  1560 th epoch : Loss in Train:  3.6425821781158447\n",
      "The  1580 th epoch : Loss in Train:  3.641573905944824\n",
      "The  1600 th epoch : Loss in Train:  3.640627384185791\n",
      "The  1620 th epoch : Loss in Train:  3.639739990234375\n",
      "The  1640 th epoch : Loss in Train:  3.6389083862304688\n",
      "The  1660 th epoch : Loss in Train:  3.6381306648254395\n",
      "The  1680 th epoch : Loss in Train:  3.637402296066284\n",
      "The  1700 th epoch : Loss in Train:  3.6367218494415283\n",
      "The  1720 th epoch : Loss in Train:  3.6360859870910645\n",
      "The  1740 th epoch : Loss in Train:  3.6354923248291016\n",
      "The  1760 th epoch : Loss in Train:  3.634939432144165\n",
      "The  1780 th epoch : Loss in Train:  3.6344239711761475\n",
      "The  1800 th epoch : Loss in Train:  3.633944034576416\n",
      "The  1820 th epoch : Loss in Train:  3.633497953414917\n",
      "The  1840 th epoch : Loss in Train:  3.6330840587615967\n",
      "The  1860 th epoch : Loss in Train:  3.6326987743377686\n",
      "The  1880 th epoch : Loss in Train:  3.6323418617248535\n",
      "The  1900 th epoch : Loss in Train:  3.6320114135742188\n",
      "The  1920 th epoch : Loss in Train:  3.6317057609558105\n",
      "The  1940 th epoch : Loss in Train:  3.631423234939575\n",
      "The  1960 th epoch : Loss in Train:  3.631161689758301\n",
      "The  1980 th epoch : Loss in Train:  3.630920886993408\n",
      "The  2000 th epoch : Loss in Train:  3.6306986808776855\n",
      "The  2020 th epoch : Loss in Train:  3.6304938793182373\n",
      "The  2040 th epoch : Loss in Train:  3.6303060054779053\n",
      "The  2060 th epoch : Loss in Train:  3.6301329135894775\n",
      "The  2080 th epoch : Loss in Train:  3.629973888397217\n",
      "The  2100 th epoch : Loss in Train:  3.629828929901123\n",
      "The  2120 th epoch : Loss in Train:  3.6296956539154053\n",
      "The  2140 th epoch : Loss in Train:  3.6295740604400635\n",
      "The  2160 th epoch : Loss in Train:  3.629462718963623\n",
      "The  2180 th epoch : Loss in Train:  3.629361629486084\n",
      "The  2200 th epoch : Loss in Train:  3.6292693614959717\n",
      "The  2220 th epoch : Loss in Train:  3.6291849613189697\n",
      "The  2240 th epoch : Loss in Train:  3.629108190536499\n",
      "The  2260 th epoch : Loss in Train:  3.6290390491485596\n",
      "The  2280 th epoch : Loss in Train:  3.6289756298065186\n",
      "The  2300 th epoch : Loss in Train:  3.6289191246032715\n",
      "The  2320 th epoch : Loss in Train:  3.6288673877716064\n",
      "The  2340 th epoch : Loss in Train:  3.6288206577301025\n",
      "The  2360 th epoch : Loss in Train:  3.6287789344787598\n",
      "The  2380 th epoch : Loss in Train:  3.6287407875061035\n",
      "The  2400 th epoch : Loss in Train:  3.628706693649292\n",
      "The  2420 th epoch : Loss in Train:  3.628676176071167\n",
      "The  2440 th epoch : Loss in Train:  3.6286487579345703\n",
      "The  2460 th epoch : Loss in Train:  3.6286239624023438\n",
      "The  2480 th epoch : Loss in Train:  3.6286017894744873\n",
      "The  2500 th epoch : Loss in Train:  3.628582715988159\n",
      "The  2520 th epoch : Loss in Train:  3.6285648345947266\n",
      "The  2540 th epoch : Loss in Train:  3.628549337387085\n",
      "The  2560 th epoch : Loss in Train:  3.628535509109497\n",
      "The  2580 th epoch : Loss in Train:  3.628523111343384\n",
      "The  2600 th epoch : Loss in Train:  3.628512382507324\n",
      "The  2620 th epoch : Loss in Train:  3.628502607345581\n",
      "The  2640 th epoch : Loss in Train:  3.6284942626953125\n",
      "The  2660 th epoch : Loss in Train:  3.6284866333007812\n",
      "The  2680 th epoch : Loss in Train:  3.6284797191619873\n",
      "The  2700 th epoch : Loss in Train:  3.628474235534668\n",
      "The  2720 th epoch : Loss in Train:  3.6284685134887695\n",
      "The  2740 th epoch : Loss in Train:  3.628464460372925\n",
      "The  2760 th epoch : Loss in Train:  3.628460645675659\n",
      "The  2780 th epoch : Loss in Train:  3.6284573078155518\n",
      "The  2800 th epoch : Loss in Train:  3.6284542083740234\n",
      "The  2820 th epoch : Loss in Train:  3.628451347351074\n",
      "The  2840 th epoch : Loss in Train:  3.6284492015838623\n",
      "The  2860 th epoch : Loss in Train:  3.6284472942352295\n",
      "The  2880 th epoch : Loss in Train:  3.6284453868865967\n",
      "The  2900 th epoch : Loss in Train:  3.628443956375122\n",
      "The  2920 th epoch : Loss in Train:  3.6284432411193848\n",
      "The  2940 th epoch : Loss in Train:  3.6284420490264893\n",
      "The  2960 th epoch : Loss in Train:  3.628441333770752\n",
      "The  2980 th epoch : Loss in Train:  3.6284401416778564\n",
      "The  3000 th epoch : Loss in Train:  3.628438949584961\n",
      "The  3020 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3040 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3060 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3080 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3100 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3120 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3140 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3160 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3180 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3200 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3220 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3240 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3260 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3280 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3300 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3320 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3340 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3360 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3380 th epoch : Loss in Train:  3.6284384727478027\n",
      "The  3400 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3420 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3440 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3460 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3480 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3500 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3520 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3540 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3560 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3580 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3600 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3620 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3640 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3660 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3680 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3700 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3720 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3740 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3760 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3780 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3800 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3820 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3840 th epoch : Loss in Train:  3.6284379959106445\n",
      "The  3860 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3880 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3900 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3920 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3940 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3960 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  3980 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4000 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4020 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4040 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4060 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4080 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4100 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4120 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4140 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4160 th epoch : Loss in Train:  3.6284372806549072\n",
      "The  4180 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4200 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4220 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4240 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4260 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4280 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4300 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4320 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4340 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4360 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4380 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4400 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4420 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4440 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4460 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4480 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4500 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4520 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4540 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4560 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4580 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4600 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4620 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4640 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4660 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4680 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4700 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4720 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4740 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4760 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4780 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4800 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4820 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4840 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4860 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4880 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4900 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4920 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4940 th epoch : Loss in Train:  3.628436803817749\n",
      "The  4960 th epoch : Loss in Train:  3.628436326980591\n",
      "The  4980 th epoch : Loss in Train:  3.628436326980591\n",
      "The  5000 th epoch : Loss in Train:  3.628436326980591\n",
      "the lamda is:  0.1 , the learning rate is:  0.003 .\n",
      "the Loss in Train data is: 3.628436326980591  Loss in Validation is:  1.8057055473327637\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.698901653289795\n",
      "The  20 th epoch : Loss in Train:  4.683380126953125\n",
      "The  40 th epoch : Loss in Train:  4.670111179351807\n",
      "The  60 th epoch : Loss in Train:  4.656032085418701\n",
      "The  80 th epoch : Loss in Train:  4.641846656799316\n",
      "The  100 th epoch : Loss in Train:  4.627931118011475\n",
      "The  120 th epoch : Loss in Train:  4.614022731781006\n",
      "The  140 th epoch : Loss in Train:  4.600213050842285\n",
      "The  160 th epoch : Loss in Train:  4.586528778076172\n",
      "The  180 th epoch : Loss in Train:  4.572980880737305\n",
      "The  200 th epoch : Loss in Train:  4.559571266174316\n",
      "The  220 th epoch : Loss in Train:  4.546298027038574\n",
      "The  240 th epoch : Loss in Train:  4.5331621170043945\n",
      "The  260 th epoch : Loss in Train:  4.520162105560303\n",
      "The  280 th epoch : Loss in Train:  4.507299900054932\n",
      "The  300 th epoch : Loss in Train:  4.494572162628174\n",
      "The  320 th epoch : Loss in Train:  4.4819817543029785\n",
      "The  340 th epoch : Loss in Train:  4.4695258140563965\n",
      "The  360 th epoch : Loss in Train:  4.457205772399902\n",
      "The  380 th epoch : Loss in Train:  4.4450201988220215\n",
      "The  400 th epoch : Loss in Train:  4.432969093322754\n",
      "The  420 th epoch : Loss in Train:  4.421051025390625\n",
      "The  440 th epoch : Loss in Train:  4.409266948699951\n",
      "The  460 th epoch : Loss in Train:  4.397615432739258\n",
      "The  480 th epoch : Loss in Train:  4.3860955238342285\n",
      "The  500 th epoch : Loss in Train:  4.374708652496338\n",
      "The  520 th epoch : Loss in Train:  4.363452434539795\n",
      "The  540 th epoch : Loss in Train:  4.3523268699646\n",
      "The  560 th epoch : Loss in Train:  4.3413310050964355\n",
      "The  580 th epoch : Loss in Train:  4.3304643630981445\n",
      "The  600 th epoch : Loss in Train:  4.319726467132568\n",
      "The  620 th epoch : Loss in Train:  4.309116363525391\n",
      "The  640 th epoch : Loss in Train:  4.298633575439453\n",
      "The  660 th epoch : Loss in Train:  4.288276672363281\n",
      "The  680 th epoch : Loss in Train:  4.278046131134033\n",
      "The  700 th epoch : Loss in Train:  4.267940044403076\n",
      "The  720 th epoch : Loss in Train:  4.257958889007568\n",
      "The  740 th epoch : Loss in Train:  4.248098850250244\n",
      "The  760 th epoch : Loss in Train:  4.238363742828369\n",
      "The  780 th epoch : Loss in Train:  4.228748798370361\n",
      "The  800 th epoch : Loss in Train:  4.219254970550537\n",
      "The  820 th epoch : Loss in Train:  4.209880828857422\n",
      "The  840 th epoch : Loss in Train:  4.200626373291016\n",
      "The  860 th epoch : Loss in Train:  4.191488742828369\n",
      "The  880 th epoch : Loss in Train:  4.182468891143799\n",
      "The  900 th epoch : Loss in Train:  4.17356538772583\n",
      "The  920 th epoch : Loss in Train:  4.164777755737305\n",
      "The  940 th epoch : Loss in Train:  4.156103134155273\n",
      "The  960 th epoch : Loss in Train:  4.147543430328369\n",
      "The  980 th epoch : Loss in Train:  4.139095306396484\n",
      "The  1000 th epoch : Loss in Train:  4.130758285522461\n",
      "The  1020 th epoch : Loss in Train:  4.122531890869141\n",
      "The  1040 th epoch : Loss in Train:  4.114415168762207\n",
      "The  1060 th epoch : Loss in Train:  4.106406211853027\n",
      "The  1080 th epoch : Loss in Train:  4.09850549697876\n",
      "The  1100 th epoch : Loss in Train:  4.0907111167907715\n",
      "The  1120 th epoch : Loss in Train:  4.083022117614746\n",
      "The  1140 th epoch : Loss in Train:  4.075437545776367\n",
      "The  1160 th epoch : Loss in Train:  4.06795597076416\n",
      "The  1180 th epoch : Loss in Train:  4.060576915740967\n",
      "The  1200 th epoch : Loss in Train:  4.053299903869629\n",
      "The  1220 th epoch : Loss in Train:  4.046122074127197\n",
      "The  1240 th epoch : Loss in Train:  4.03904390335083\n",
      "The  1260 th epoch : Loss in Train:  4.032063007354736\n",
      "The  1280 th epoch : Loss in Train:  4.025180816650391\n",
      "The  1300 th epoch : Loss in Train:  4.0183939933776855\n",
      "The  1320 th epoch : Loss in Train:  4.011702537536621\n",
      "The  1340 th epoch : Loss in Train:  4.005105495452881\n",
      "The  1360 th epoch : Loss in Train:  3.9986014366149902\n",
      "The  1380 th epoch : Loss in Train:  3.992189407348633\n",
      "The  1400 th epoch : Loss in Train:  3.9858691692352295\n",
      "The  1420 th epoch : Loss in Train:  3.979637861251831\n",
      "The  1440 th epoch : Loss in Train:  3.9734950065612793\n",
      "The  1460 th epoch : Loss in Train:  3.9674408435821533\n",
      "The  1480 th epoch : Loss in Train:  3.961472749710083\n",
      "The  1500 th epoch : Loss in Train:  3.9555912017822266\n",
      "The  1520 th epoch : Loss in Train:  3.9497945308685303\n",
      "The  1540 th epoch : Loss in Train:  3.9440817832946777\n",
      "The  1560 th epoch : Loss in Train:  3.9384524822235107\n",
      "The  1580 th epoch : Loss in Train:  3.9329047203063965\n",
      "The  1600 th epoch : Loss in Train:  3.927438497543335\n",
      "The  1620 th epoch : Loss in Train:  3.9220521450042725\n",
      "The  1640 th epoch : Loss in Train:  3.9167449474334717\n",
      "The  1660 th epoch : Loss in Train:  3.911515951156616\n",
      "The  1680 th epoch : Loss in Train:  3.9063632488250732\n",
      "The  1700 th epoch : Loss in Train:  3.901287317276001\n",
      "The  1720 th epoch : Loss in Train:  3.8962864875793457\n",
      "The  1740 th epoch : Loss in Train:  3.891359329223633\n",
      "The  1760 th epoch : Loss in Train:  3.886507034301758\n",
      "The  1780 th epoch : Loss in Train:  3.8817267417907715\n",
      "The  1800 th epoch : Loss in Train:  3.877018928527832\n",
      "The  1820 th epoch : Loss in Train:  3.872380256652832\n",
      "The  1840 th epoch : Loss in Train:  3.867811918258667\n",
      "The  1860 th epoch : Loss in Train:  3.8633124828338623\n",
      "The  1880 th epoch : Loss in Train:  3.8588814735412598\n",
      "The  1900 th epoch : Loss in Train:  3.854517936706543\n",
      "The  1920 th epoch : Loss in Train:  3.850219249725342\n",
      "The  1940 th epoch : Loss in Train:  3.845987319946289\n",
      "The  1960 th epoch : Loss in Train:  3.841820478439331\n",
      "The  1980 th epoch : Loss in Train:  3.837716817855835\n",
      "The  2000 th epoch : Loss in Train:  3.833676338195801\n",
      "The  2020 th epoch : Loss in Train:  3.829697847366333\n",
      "The  2040 th epoch : Loss in Train:  3.8257808685302734\n",
      "The  2060 th epoch : Loss in Train:  3.8219242095947266\n",
      "The  2080 th epoch : Loss in Train:  3.8181281089782715\n",
      "The  2100 th epoch : Loss in Train:  3.8143904209136963\n",
      "The  2120 th epoch : Loss in Train:  3.810711622238159\n",
      "The  2140 th epoch : Loss in Train:  3.8070900440216064\n",
      "The  2160 th epoch : Loss in Train:  3.803524971008301\n",
      "The  2180 th epoch : Loss in Train:  3.8000166416168213\n",
      "The  2200 th epoch : Loss in Train:  3.796562910079956\n",
      "The  2220 th epoch : Loss in Train:  3.7931647300720215\n",
      "The  2240 th epoch : Loss in Train:  3.7898194789886475\n",
      "The  2260 th epoch : Loss in Train:  3.7865278720855713\n",
      "The  2280 th epoch : Loss in Train:  3.7832889556884766\n",
      "The  2300 th epoch : Loss in Train:  3.7801015377044678\n",
      "The  2320 th epoch : Loss in Train:  3.776965379714966\n",
      "The  2340 th epoch : Loss in Train:  3.7738800048828125\n",
      "The  2360 th epoch : Loss in Train:  3.7708442211151123\n",
      "The  2380 th epoch : Loss in Train:  3.767857551574707\n",
      "The  2400 th epoch : Loss in Train:  3.7649195194244385\n",
      "The  2420 th epoch : Loss in Train:  3.7620296478271484\n",
      "The  2440 th epoch : Loss in Train:  3.7591869831085205\n",
      "The  2460 th epoch : Loss in Train:  3.7563910484313965\n",
      "The  2480 th epoch : Loss in Train:  3.7536404132843018\n",
      "The  2500 th epoch : Loss in Train:  3.750936508178711\n",
      "The  2520 th epoch : Loss in Train:  3.748276472091675\n",
      "The  2540 th epoch : Loss in Train:  3.7456607818603516\n",
      "The  2560 th epoch : Loss in Train:  3.743088960647583\n",
      "The  2580 th epoch : Loss in Train:  3.740560531616211\n",
      "The  2600 th epoch : Loss in Train:  3.738074779510498\n",
      "The  2620 th epoch : Loss in Train:  3.735630989074707\n",
      "The  2640 th epoch : Loss in Train:  3.733228921890259\n",
      "The  2660 th epoch : Loss in Train:  3.7308671474456787\n",
      "The  2680 th epoch : Loss in Train:  3.728546619415283\n",
      "The  2700 th epoch : Loss in Train:  3.7262656688690186\n",
      "The  2720 th epoch : Loss in Train:  3.7240242958068848\n",
      "The  2740 th epoch : Loss in Train:  3.7218210697174072\n",
      "The  2760 th epoch : Loss in Train:  3.7196571826934814\n",
      "The  2780 th epoch : Loss in Train:  3.7175304889678955\n",
      "The  2800 th epoch : Loss in Train:  3.7154417037963867\n",
      "The  2820 th epoch : Loss in Train:  3.7133896350860596\n",
      "The  2840 th epoch : Loss in Train:  3.711374044418335\n",
      "The  2860 th epoch : Loss in Train:  3.709394693374634\n",
      "The  2880 th epoch : Loss in Train:  3.7074503898620605\n",
      "The  2900 th epoch : Loss in Train:  3.7055416107177734\n",
      "The  2920 th epoch : Loss in Train:  3.703667163848877\n",
      "The  2940 th epoch : Loss in Train:  3.701826810836792\n",
      "The  2960 th epoch : Loss in Train:  3.7000203132629395\n",
      "The  2980 th epoch : Loss in Train:  3.6982474327087402\n",
      "The  3000 th epoch : Loss in Train:  3.6965067386627197\n",
      "The  3020 th epoch : Loss in Train:  3.6947989463806152\n",
      "The  3040 th epoch : Loss in Train:  3.6931228637695312\n",
      "The  3060 th epoch : Loss in Train:  3.6914784908294678\n",
      "The  3080 th epoch : Loss in Train:  3.6898655891418457\n",
      "The  3100 th epoch : Loss in Train:  3.6882829666137695\n",
      "The  3120 th epoch : Loss in Train:  3.6867311000823975\n",
      "The  3140 th epoch : Loss in Train:  3.685209274291992\n",
      "The  3160 th epoch : Loss in Train:  3.6837170124053955\n",
      "The  3180 th epoch : Loss in Train:  3.682253837585449\n",
      "The  3200 th epoch : Loss in Train:  3.6808197498321533\n",
      "The  3220 th epoch : Loss in Train:  3.6794135570526123\n",
      "The  3240 th epoch : Loss in Train:  3.6780359745025635\n",
      "The  3260 th epoch : Loss in Train:  3.6766860485076904\n",
      "The  3280 th epoch : Loss in Train:  3.675363540649414\n",
      "The  3300 th epoch : Loss in Train:  3.6740682125091553\n",
      "The  3320 th epoch : Loss in Train:  3.6727988719940186\n",
      "The  3340 th epoch : Loss in Train:  3.671556234359741\n",
      "The  3360 th epoch : Loss in Train:  3.6703391075134277\n",
      "The  3380 th epoch : Loss in Train:  3.6691482067108154\n",
      "The  3400 th epoch : Loss in Train:  3.6679821014404297\n",
      "The  3420 th epoch : Loss in Train:  3.6668412685394287\n",
      "The  3440 th epoch : Loss in Train:  3.665724039077759\n",
      "The  3460 th epoch : Loss in Train:  3.6646318435668945\n",
      "The  3480 th epoch : Loss in Train:  3.6635634899139404\n",
      "The  3500 th epoch : Loss in Train:  3.6625185012817383\n",
      "The  3520 th epoch : Loss in Train:  3.661496639251709\n",
      "The  3540 th epoch : Loss in Train:  3.6604976654052734\n",
      "The  3560 th epoch : Loss in Train:  3.6595213413238525\n",
      "The  3580 th epoch : Loss in Train:  3.658567428588867\n",
      "The  3600 th epoch : Loss in Train:  3.657635450363159\n",
      "The  3620 th epoch : Loss in Train:  3.6567249298095703\n",
      "The  3640 th epoch : Loss in Train:  3.6558356285095215\n",
      "The  3660 th epoch : Loss in Train:  3.6549675464630127\n",
      "The  3680 th epoch : Loss in Train:  3.6541197299957275\n",
      "The  3700 th epoch : Loss in Train:  3.653292655944824\n",
      "The  3720 th epoch : Loss in Train:  3.6524851322174072\n",
      "The  3740 th epoch : Loss in Train:  3.651698112487793\n",
      "The  3760 th epoch : Loss in Train:  3.6509299278259277\n",
      "The  3780 th epoch : Loss in Train:  3.650181531906128\n",
      "The  3800 th epoch : Loss in Train:  3.649451494216919\n",
      "The  3820 th epoch : Loss in Train:  3.6487395763397217\n",
      "The  3840 th epoch : Loss in Train:  3.6480467319488525\n",
      "The  3860 th epoch : Loss in Train:  3.647371292114258\n",
      "The  3880 th epoch : Loss in Train:  3.6467137336730957\n",
      "The  3900 th epoch : Loss in Train:  3.646073579788208\n",
      "The  3920 th epoch : Loss in Train:  3.6454503536224365\n",
      "The  3940 th epoch : Loss in Train:  3.6448440551757812\n",
      "The  3960 th epoch : Loss in Train:  3.644254207611084\n",
      "The  3980 th epoch : Loss in Train:  3.6436803340911865\n",
      "The  4000 th epoch : Loss in Train:  3.643122911453247\n",
      "The  4020 th epoch : Loss in Train:  3.642580509185791\n",
      "The  4040 th epoch : Loss in Train:  3.642054557800293\n",
      "The  4060 th epoch : Loss in Train:  3.641542434692383\n",
      "The  4080 th epoch : Loss in Train:  3.6410462856292725\n",
      "The  4100 th epoch : Loss in Train:  3.640563488006592\n",
      "The  4120 th epoch : Loss in Train:  3.6400957107543945\n",
      "The  4140 th epoch : Loss in Train:  3.6396420001983643\n",
      "The  4160 th epoch : Loss in Train:  3.6392014026641846\n",
      "The  4180 th epoch : Loss in Train:  3.6387746334075928\n",
      "The  4200 th epoch : Loss in Train:  3.6383609771728516\n",
      "The  4220 th epoch : Loss in Train:  3.63796067237854\n",
      "The  4240 th epoch : Loss in Train:  3.6375725269317627\n",
      "The  4260 th epoch : Loss in Train:  3.6371965408325195\n",
      "The  4280 th epoch : Loss in Train:  3.6368331909179688\n",
      "The  4300 th epoch : Loss in Train:  3.636481523513794\n",
      "The  4320 th epoch : Loss in Train:  3.6361405849456787\n",
      "The  4340 th epoch : Loss in Train:  3.6358120441436768\n",
      "The  4360 th epoch : Loss in Train:  3.6354939937591553\n",
      "The  4380 th epoch : Loss in Train:  3.6351871490478516\n",
      "The  4400 th epoch : Loss in Train:  3.6348912715911865\n",
      "The  4420 th epoch : Loss in Train:  3.6346049308776855\n",
      "The  4440 th epoch : Loss in Train:  3.6343295574188232\n",
      "The  4460 th epoch : Loss in Train:  3.634063243865967\n",
      "The  4480 th epoch : Loss in Train:  3.6338067054748535\n",
      "The  4500 th epoch : Loss in Train:  3.633559465408325\n",
      "The  4520 th epoch : Loss in Train:  3.633321762084961\n",
      "The  4540 th epoch : Loss in Train:  3.6330924034118652\n",
      "The  4560 th epoch : Loss in Train:  3.6328718662261963\n",
      "The  4580 th epoch : Loss in Train:  3.632659912109375\n",
      "The  4600 th epoch : Loss in Train:  3.6324563026428223\n",
      "The  4620 th epoch : Loss in Train:  3.632260322570801\n",
      "The  4640 th epoch : Loss in Train:  3.6320724487304688\n",
      "The  4660 th epoch : Loss in Train:  3.6318917274475098\n",
      "The  4680 th epoch : Loss in Train:  3.631718397140503\n",
      "The  4700 th epoch : Loss in Train:  3.63155198097229\n",
      "The  4720 th epoch : Loss in Train:  3.63139271736145\n",
      "The  4740 th epoch : Loss in Train:  3.631240129470825\n",
      "The  4760 th epoch : Loss in Train:  3.631093978881836\n",
      "The  4780 th epoch : Loss in Train:  3.630953788757324\n",
      "The  4800 th epoch : Loss in Train:  3.6308200359344482\n",
      "The  4820 th epoch : Loss in Train:  3.6306920051574707\n",
      "The  4840 th epoch : Loss in Train:  3.6305699348449707\n",
      "The  4860 th epoch : Loss in Train:  3.630453109741211\n",
      "The  4880 th epoch : Loss in Train:  3.6303412914276123\n",
      "The  4900 th epoch : Loss in Train:  3.630234956741333\n",
      "The  4920 th epoch : Loss in Train:  3.630133867263794\n",
      "The  4940 th epoch : Loss in Train:  3.6300370693206787\n",
      "The  4960 th epoch : Loss in Train:  3.6299445629119873\n",
      "The  4980 th epoch : Loss in Train:  3.629857063293457\n",
      "The  5000 th epoch : Loss in Train:  3.6297740936279297\n",
      "the lamda is:  0.1 , the learning rate is:  0.001 .\n",
      "the Loss in Train data is: 3.6297740936279297  Loss in Validation is:  1.806829810142517\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.700338363647461\n",
      "The  20 th epoch : Loss in Train:  4.698983192443848\n",
      "The  40 th epoch : Loss in Train:  4.697453022003174\n",
      "The  60 th epoch : Loss in Train:  4.696132183074951\n",
      "The  80 th epoch : Loss in Train:  4.694654941558838\n",
      "The  100 th epoch : Loss in Train:  4.693155765533447\n",
      "The  120 th epoch : Loss in Train:  4.691453456878662\n",
      "The  140 th epoch : Loss in Train:  4.689477443695068\n",
      "The  160 th epoch : Loss in Train:  4.688676357269287\n",
      "The  180 th epoch : Loss in Train:  4.687130928039551\n",
      "The  200 th epoch : Loss in Train:  4.6858367919921875\n",
      "The  220 th epoch : Loss in Train:  4.684164047241211\n",
      "The  240 th epoch : Loss in Train:  4.683040618896484\n",
      "The  260 th epoch : Loss in Train:  4.680929660797119\n",
      "The  280 th epoch : Loss in Train:  4.679991722106934\n",
      "The  300 th epoch : Loss in Train:  4.678371429443359\n",
      "The  320 th epoch : Loss in Train:  4.676892280578613\n",
      "The  340 th epoch : Loss in Train:  4.675550937652588\n",
      "The  360 th epoch : Loss in Train:  4.6741943359375\n",
      "The  380 th epoch : Loss in Train:  4.672847747802734\n",
      "The  400 th epoch : Loss in Train:  4.671296119689941\n",
      "The  420 th epoch : Loss in Train:  4.669919013977051\n",
      "The  440 th epoch : Loss in Train:  4.6685261726379395\n",
      "The  460 th epoch : Loss in Train:  4.667156219482422\n",
      "The  480 th epoch : Loss in Train:  4.665707111358643\n",
      "The  500 th epoch : Loss in Train:  4.664267063140869\n",
      "The  520 th epoch : Loss in Train:  4.662792682647705\n",
      "The  540 th epoch : Loss in Train:  4.661425590515137\n",
      "The  560 th epoch : Loss in Train:  4.659886837005615\n",
      "The  580 th epoch : Loss in Train:  4.658541679382324\n",
      "The  600 th epoch : Loss in Train:  4.65718412399292\n",
      "The  620 th epoch : Loss in Train:  4.655706405639648\n",
      "The  640 th epoch : Loss in Train:  4.654268264770508\n",
      "The  660 th epoch : Loss in Train:  4.652843952178955\n",
      "The  680 th epoch : Loss in Train:  4.651486396789551\n",
      "The  700 th epoch : Loss in Train:  4.649984836578369\n",
      "The  720 th epoch : Loss in Train:  4.6486101150512695\n",
      "The  740 th epoch : Loss in Train:  4.647200107574463\n",
      "The  760 th epoch : Loss in Train:  4.645806789398193\n",
      "The  780 th epoch : Loss in Train:  4.6443915367126465\n",
      "The  800 th epoch : Loss in Train:  4.642969131469727\n",
      "The  820 th epoch : Loss in Train:  4.6415696144104\n",
      "The  840 th epoch : Loss in Train:  4.640130519866943\n",
      "The  860 th epoch : Loss in Train:  4.638733386993408\n",
      "The  880 th epoch : Loss in Train:  4.637302398681641\n",
      "The  900 th epoch : Loss in Train:  4.635912895202637\n",
      "The  920 th epoch : Loss in Train:  4.63450813293457\n",
      "The  940 th epoch : Loss in Train:  4.633105754852295\n",
      "The  960 th epoch : Loss in Train:  4.631702899932861\n",
      "The  980 th epoch : Loss in Train:  4.630298614501953\n",
      "The  1000 th epoch : Loss in Train:  4.628887176513672\n",
      "The  1020 th epoch : Loss in Train:  4.627490997314453\n",
      "The  1040 th epoch : Loss in Train:  4.626089096069336\n",
      "The  1060 th epoch : Loss in Train:  4.624695301055908\n",
      "The  1080 th epoch : Loss in Train:  4.623287677764893\n",
      "The  1100 th epoch : Loss in Train:  4.6218976974487305\n",
      "The  1120 th epoch : Loss in Train:  4.620500087738037\n",
      "The  1140 th epoch : Loss in Train:  4.619107723236084\n",
      "The  1160 th epoch : Loss in Train:  4.617713928222656\n",
      "The  1180 th epoch : Loss in Train:  4.616321563720703\n",
      "The  1200 th epoch : Loss in Train:  4.614930629730225\n",
      "The  1220 th epoch : Loss in Train:  4.613537788391113\n",
      "The  1240 th epoch : Loss in Train:  4.6121506690979\n",
      "The  1260 th epoch : Loss in Train:  4.610762119293213\n",
      "The  1280 th epoch : Loss in Train:  4.609373569488525\n",
      "The  1300 th epoch : Loss in Train:  4.60798978805542\n",
      "The  1320 th epoch : Loss in Train:  4.606605529785156\n",
      "The  1340 th epoch : Loss in Train:  4.605222225189209\n",
      "The  1360 th epoch : Loss in Train:  4.60383939743042\n",
      "The  1380 th epoch : Loss in Train:  4.602458953857422\n",
      "The  1400 th epoch : Loss in Train:  4.601078987121582\n",
      "The  1420 th epoch : Loss in Train:  4.599700450897217\n",
      "The  1440 th epoch : Loss in Train:  4.598322868347168\n",
      "The  1460 th epoch : Loss in Train:  4.596946716308594\n",
      "The  1480 th epoch : Loss in Train:  4.595571517944336\n",
      "The  1500 th epoch : Loss in Train:  4.5941972732543945\n",
      "The  1520 th epoch : Loss in Train:  4.592824459075928\n",
      "The  1540 th epoch : Loss in Train:  4.591452121734619\n",
      "The  1560 th epoch : Loss in Train:  4.590081691741943\n",
      "The  1580 th epoch : Loss in Train:  4.588712215423584\n",
      "The  1600 th epoch : Loss in Train:  4.587343692779541\n",
      "The  1620 th epoch : Loss in Train:  4.585976600646973\n",
      "The  1640 th epoch : Loss in Train:  4.584610939025879\n",
      "The  1660 th epoch : Loss in Train:  4.583245277404785\n",
      "The  1680 th epoch : Loss in Train:  4.581881523132324\n",
      "The  1700 th epoch : Loss in Train:  4.58051872253418\n",
      "The  1720 th epoch : Loss in Train:  4.579156398773193\n",
      "The  1740 th epoch : Loss in Train:  4.577796459197998\n",
      "The  1760 th epoch : Loss in Train:  4.5764360427856445\n",
      "The  1780 th epoch : Loss in Train:  4.575078964233398\n",
      "The  1800 th epoch : Loss in Train:  4.573720932006836\n",
      "The  1820 th epoch : Loss in Train:  4.572364807128906\n",
      "The  1840 th epoch : Loss in Train:  4.571009635925293\n",
      "The  1860 th epoch : Loss in Train:  4.569655895233154\n",
      "The  1880 th epoch : Loss in Train:  4.56830358505249\n",
      "The  1900 th epoch : Loss in Train:  4.566951751708984\n",
      "The  1920 th epoch : Loss in Train:  4.565600872039795\n",
      "The  1940 th epoch : Loss in Train:  4.564250946044922\n",
      "The  1960 th epoch : Loss in Train:  4.562902927398682\n",
      "The  1980 th epoch : Loss in Train:  4.5615553855896\n",
      "The  2000 th epoch : Loss in Train:  4.560208797454834\n",
      "The  2020 th epoch : Loss in Train:  4.558864593505859\n",
      "The  2040 th epoch : Loss in Train:  4.557519912719727\n",
      "The  2060 th epoch : Loss in Train:  4.556177139282227\n",
      "The  2080 th epoch : Loss in Train:  4.554835319519043\n",
      "The  2100 th epoch : Loss in Train:  4.553493976593018\n",
      "The  2120 th epoch : Loss in Train:  4.552155017852783\n",
      "The  2140 th epoch : Loss in Train:  4.550815582275391\n",
      "The  2160 th epoch : Loss in Train:  4.549478054046631\n",
      "The  2180 th epoch : Loss in Train:  4.5481414794921875\n",
      "The  2200 th epoch : Loss in Train:  4.5468058586120605\n",
      "The  2220 th epoch : Loss in Train:  4.545471668243408\n",
      "The  2240 th epoch : Loss in Train:  4.544138431549072\n",
      "The  2260 th epoch : Loss in Train:  4.542806148529053\n",
      "The  2280 th epoch : Loss in Train:  4.54147481918335\n",
      "The  2300 th epoch : Loss in Train:  4.540144443511963\n",
      "The  2320 th epoch : Loss in Train:  4.538815975189209\n",
      "The  2340 th epoch : Loss in Train:  4.537487506866455\n",
      "The  2360 th epoch : Loss in Train:  4.536160469055176\n",
      "The  2380 th epoch : Loss in Train:  4.534834384918213\n",
      "The  2400 th epoch : Loss in Train:  4.533509731292725\n",
      "The  2420 th epoch : Loss in Train:  4.532186508178711\n",
      "The  2440 th epoch : Loss in Train:  4.530863285064697\n",
      "The  2460 th epoch : Loss in Train:  4.529541969299316\n",
      "The  2480 th epoch : Loss in Train:  4.5282206535339355\n",
      "The  2500 th epoch : Loss in Train:  4.526900768280029\n",
      "The  2520 th epoch : Loss in Train:  4.525582790374756\n",
      "The  2540 th epoch : Loss in Train:  4.524265289306641\n",
      "The  2560 th epoch : Loss in Train:  4.522948265075684\n",
      "The  2580 th epoch : Loss in Train:  4.521633148193359\n",
      "The  2600 th epoch : Loss in Train:  4.520318508148193\n",
      "The  2620 th epoch : Loss in Train:  4.519004821777344\n",
      "The  2640 th epoch : Loss in Train:  4.517693042755127\n",
      "The  2660 th epoch : Loss in Train:  4.516381740570068\n",
      "The  2680 th epoch : Loss in Train:  4.515070915222168\n",
      "The  2700 th epoch : Loss in Train:  4.5137619972229\n",
      "The  2720 th epoch : Loss in Train:  4.512453556060791\n",
      "The  2740 th epoch : Loss in Train:  4.511146545410156\n",
      "The  2760 th epoch : Loss in Train:  4.509840488433838\n",
      "The  2780 th epoch : Loss in Train:  4.508535385131836\n",
      "The  2800 th epoch : Loss in Train:  4.507230758666992\n",
      "The  2820 th epoch : Loss in Train:  4.505927562713623\n",
      "The  2840 th epoch : Loss in Train:  4.5046257972717285\n",
      "The  2860 th epoch : Loss in Train:  4.50332498550415\n",
      "The  2880 th epoch : Loss in Train:  4.5020246505737305\n",
      "The  2900 th epoch : Loss in Train:  4.500725746154785\n",
      "The  2920 th epoch : Loss in Train:  4.499427318572998\n",
      "The  2940 th epoch : Loss in Train:  4.498130798339844\n",
      "The  2960 th epoch : Loss in Train:  4.496835231781006\n",
      "The  2980 th epoch : Loss in Train:  4.495539665222168\n",
      "The  3000 th epoch : Loss in Train:  4.494245529174805\n",
      "The  3020 th epoch : Loss in Train:  4.492952346801758\n",
      "The  3040 th epoch : Loss in Train:  4.491661071777344\n",
      "The  3060 th epoch : Loss in Train:  4.490370273590088\n",
      "The  3080 th epoch : Loss in Train:  4.48907995223999\n",
      "The  3100 th epoch : Loss in Train:  4.487790584564209\n",
      "The  3120 th epoch : Loss in Train:  4.486502170562744\n",
      "The  3140 th epoch : Loss in Train:  4.485215663909912\n",
      "The  3160 th epoch : Loss in Train:  4.483930587768555\n",
      "The  3180 th epoch : Loss in Train:  4.482644557952881\n",
      "The  3200 th epoch : Loss in Train:  4.481360912322998\n",
      "The  3220 th epoch : Loss in Train:  4.480077743530273\n",
      "The  3240 th epoch : Loss in Train:  4.478796005249023\n",
      "The  3260 th epoch : Loss in Train:  4.47751522064209\n",
      "The  3280 th epoch : Loss in Train:  4.476235389709473\n",
      "The  3300 th epoch : Loss in Train:  4.474956512451172\n",
      "The  3320 th epoch : Loss in Train:  4.473678112030029\n",
      "The  3340 th epoch : Loss in Train:  4.472401142120361\n",
      "The  3360 th epoch : Loss in Train:  4.471125602722168\n",
      "The  3380 th epoch : Loss in Train:  4.469850540161133\n",
      "The  3400 th epoch : Loss in Train:  4.468576431274414\n",
      "The  3420 th epoch : Loss in Train:  4.467304229736328\n",
      "The  3440 th epoch : Loss in Train:  4.466032028198242\n",
      "The  3460 th epoch : Loss in Train:  4.4647603034973145\n",
      "The  3480 th epoch : Loss in Train:  4.4634904861450195\n",
      "The  3500 th epoch : Loss in Train:  4.462222099304199\n",
      "The  3520 th epoch : Loss in Train:  4.460953712463379\n",
      "The  3540 th epoch : Loss in Train:  4.459686279296875\n",
      "The  3560 th epoch : Loss in Train:  4.458420753479004\n",
      "The  3580 th epoch : Loss in Train:  4.457155227661133\n",
      "The  3600 th epoch : Loss in Train:  4.455891132354736\n",
      "The  3620 th epoch : Loss in Train:  4.454627990722656\n",
      "The  3640 th epoch : Loss in Train:  4.453365325927734\n",
      "The  3660 th epoch : Loss in Train:  4.452104568481445\n",
      "The  3680 th epoch : Loss in Train:  4.4508442878723145\n",
      "The  3700 th epoch : Loss in Train:  4.4495849609375\n",
      "The  3720 th epoch : Loss in Train:  4.448327541351318\n",
      "The  3740 th epoch : Loss in Train:  4.4470696449279785\n",
      "The  3760 th epoch : Loss in Train:  4.4458136558532715\n",
      "The  3780 th epoch : Loss in Train:  4.444558143615723\n",
      "The  3800 th epoch : Loss in Train:  4.443304061889648\n",
      "The  3820 th epoch : Loss in Train:  4.442050933837891\n",
      "The  3840 th epoch : Loss in Train:  4.440798759460449\n",
      "The  3860 th epoch : Loss in Train:  4.439547061920166\n",
      "The  3880 th epoch : Loss in Train:  4.438296318054199\n",
      "The  3900 th epoch : Loss in Train:  4.437047004699707\n",
      "The  3920 th epoch : Loss in Train:  4.435798645019531\n",
      "The  3940 th epoch : Loss in Train:  4.434551239013672\n",
      "The  3960 th epoch : Loss in Train:  4.433304309844971\n",
      "The  3980 th epoch : Loss in Train:  4.432058811187744\n",
      "The  4000 th epoch : Loss in Train:  4.430814266204834\n",
      "The  4020 th epoch : Loss in Train:  4.429571151733398\n",
      "The  4040 th epoch : Loss in Train:  4.428328037261963\n",
      "The  4060 th epoch : Loss in Train:  4.427086353302002\n",
      "The  4080 th epoch : Loss in Train:  4.425845623016357\n",
      "The  4100 th epoch : Loss in Train:  4.424605846405029\n",
      "The  4120 th epoch : Loss in Train:  4.423367023468018\n",
      "The  4140 th epoch : Loss in Train:  4.422129154205322\n",
      "The  4160 th epoch : Loss in Train:  4.420892238616943\n",
      "The  4180 th epoch : Loss in Train:  4.419656276702881\n",
      "The  4200 th epoch : Loss in Train:  4.418422222137451\n",
      "The  4220 th epoch : Loss in Train:  4.4171881675720215\n",
      "The  4240 th epoch : Loss in Train:  4.41595458984375\n",
      "The  4260 th epoch : Loss in Train:  4.414722442626953\n",
      "The  4280 th epoch : Loss in Train:  4.413491725921631\n",
      "The  4300 th epoch : Loss in Train:  4.412261486053467\n",
      "The  4320 th epoch : Loss in Train:  4.411032199859619\n",
      "The  4340 th epoch : Loss in Train:  4.409804344177246\n",
      "The  4360 th epoch : Loss in Train:  4.408576965332031\n",
      "The  4380 th epoch : Loss in Train:  4.407350540161133\n",
      "The  4400 th epoch : Loss in Train:  4.406126022338867\n",
      "The  4420 th epoch : Loss in Train:  4.404901027679443\n",
      "The  4440 th epoch : Loss in Train:  4.403677463531494\n",
      "The  4460 th epoch : Loss in Train:  4.4024553298950195\n",
      "The  4480 th epoch : Loss in Train:  4.401233673095703\n",
      "The  4500 th epoch : Loss in Train:  4.400013446807861\n",
      "The  4520 th epoch : Loss in Train:  4.398794651031494\n",
      "The  4540 th epoch : Loss in Train:  4.397575855255127\n",
      "The  4560 th epoch : Loss in Train:  4.396358489990234\n",
      "The  4580 th epoch : Loss in Train:  4.395141124725342\n",
      "The  4600 th epoch : Loss in Train:  4.39392614364624\n",
      "The  4620 th epoch : Loss in Train:  4.392711162567139\n",
      "The  4640 th epoch : Loss in Train:  4.391497611999512\n",
      "The  4660 th epoch : Loss in Train:  4.390285015106201\n",
      "The  4680 th epoch : Loss in Train:  4.389072418212891\n",
      "The  4700 th epoch : Loss in Train:  4.387861728668213\n",
      "The  4720 th epoch : Loss in Train:  4.386651992797852\n",
      "The  4740 th epoch : Loss in Train:  4.385443210601807\n",
      "The  4760 th epoch : Loss in Train:  4.384235382080078\n",
      "The  4780 th epoch : Loss in Train:  4.383028507232666\n",
      "The  4800 th epoch : Loss in Train:  4.381822109222412\n",
      "The  4820 th epoch : Loss in Train:  4.380617141723633\n",
      "The  4840 th epoch : Loss in Train:  4.379413604736328\n",
      "The  4860 th epoch : Loss in Train:  4.378209590911865\n",
      "The  4880 th epoch : Loss in Train:  4.377007961273193\n",
      "The  4900 th epoch : Loss in Train:  4.3758063316345215\n",
      "The  4920 th epoch : Loss in Train:  4.374605178833008\n",
      "The  4940 th epoch : Loss in Train:  4.373406887054443\n",
      "The  4960 th epoch : Loss in Train:  4.372208118438721\n",
      "The  4980 th epoch : Loss in Train:  4.371011257171631\n",
      "The  5000 th epoch : Loss in Train:  4.369813919067383\n",
      "the lamda is:  0.1 , the learning rate is:  0.0001 .\n",
      "the Loss in Train data is: 4.369813919067383  Loss in Validation is:  2.438229560852051\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.700430870056152\n",
      "The  20 th epoch : Loss in Train:  4.699361801147461\n",
      "The  40 th epoch : Loss in Train:  4.697953224182129\n",
      "The  60 th epoch : Loss in Train:  4.696807861328125\n",
      "The  80 th epoch : Loss in Train:  4.6959710121154785\n",
      "The  100 th epoch : Loss in Train:  4.694530487060547\n",
      "The  120 th epoch : Loss in Train:  4.693400859832764\n",
      "The  140 th epoch : Loss in Train:  4.691958904266357\n",
      "The  160 th epoch : Loss in Train:  4.691357135772705\n",
      "The  180 th epoch : Loss in Train:  4.690593242645264\n",
      "The  200 th epoch : Loss in Train:  4.689048767089844\n",
      "The  220 th epoch : Loss in Train:  4.6883158683776855\n",
      "The  240 th epoch : Loss in Train:  4.687173366546631\n",
      "The  260 th epoch : Loss in Train:  4.685555934906006\n",
      "The  280 th epoch : Loss in Train:  4.684648513793945\n",
      "The  300 th epoch : Loss in Train:  4.682844638824463\n",
      "The  320 th epoch : Loss in Train:  4.682247161865234\n",
      "The  340 th epoch : Loss in Train:  4.681482791900635\n",
      "The  360 th epoch : Loss in Train:  4.679600715637207\n",
      "The  380 th epoch : Loss in Train:  4.6788740158081055\n",
      "The  400 th epoch : Loss in Train:  4.67821741104126\n",
      "The  420 th epoch : Loss in Train:  4.677300930023193\n",
      "The  440 th epoch : Loss in Train:  4.675634860992432\n",
      "The  460 th epoch : Loss in Train:  4.675243854522705\n",
      "The  480 th epoch : Loss in Train:  4.673344612121582\n",
      "The  500 th epoch : Loss in Train:  4.6726484298706055\n",
      "The  520 th epoch : Loss in Train:  4.671848297119141\n",
      "The  540 th epoch : Loss in Train:  4.670007228851318\n",
      "The  560 th epoch : Loss in Train:  4.669656753540039\n",
      "The  580 th epoch : Loss in Train:  4.668359279632568\n",
      "The  600 th epoch : Loss in Train:  4.666841506958008\n",
      "The  620 th epoch : Loss in Train:  4.666138648986816\n",
      "The  640 th epoch : Loss in Train:  4.664380073547363\n",
      "The  660 th epoch : Loss in Train:  4.664076328277588\n",
      "The  680 th epoch : Loss in Train:  4.663296222686768\n",
      "The  700 th epoch : Loss in Train:  4.662001132965088\n",
      "The  720 th epoch : Loss in Train:  4.6610894203186035\n",
      "The  740 th epoch : Loss in Train:  4.659786701202393\n",
      "The  760 th epoch : Loss in Train:  4.658852577209473\n",
      "The  780 th epoch : Loss in Train:  4.658027172088623\n",
      "The  800 th epoch : Loss in Train:  4.656409740447998\n",
      "The  820 th epoch : Loss in Train:  4.655823707580566\n",
      "The  840 th epoch : Loss in Train:  4.654729843139648\n",
      "The  860 th epoch : Loss in Train:  4.6536030769348145\n",
      "The  880 th epoch : Loss in Train:  4.652474880218506\n",
      "The  900 th epoch : Loss in Train:  4.651285171508789\n",
      "The  920 th epoch : Loss in Train:  4.650395393371582\n",
      "The  940 th epoch : Loss in Train:  4.649499416351318\n",
      "The  960 th epoch : Loss in Train:  4.648486614227295\n",
      "The  980 th epoch : Loss in Train:  4.647278308868408\n",
      "The  1000 th epoch : Loss in Train:  4.646207332611084\n",
      "The  1020 th epoch : Loss in Train:  4.645144462585449\n",
      "The  1040 th epoch : Loss in Train:  4.644137859344482\n",
      "The  1060 th epoch : Loss in Train:  4.6431756019592285\n",
      "The  1080 th epoch : Loss in Train:  4.641983509063721\n",
      "The  1100 th epoch : Loss in Train:  4.64096212387085\n",
      "The  1120 th epoch : Loss in Train:  4.639942169189453\n",
      "The  1140 th epoch : Loss in Train:  4.638901233673096\n",
      "The  1160 th epoch : Loss in Train:  4.63783597946167\n",
      "The  1180 th epoch : Loss in Train:  4.6368207931518555\n",
      "The  1200 th epoch : Loss in Train:  4.635749340057373\n",
      "The  1220 th epoch : Loss in Train:  4.634698390960693\n",
      "The  1240 th epoch : Loss in Train:  4.633659362792969\n",
      "The  1260 th epoch : Loss in Train:  4.6325883865356445\n",
      "The  1280 th epoch : Loss in Train:  4.6315436363220215\n",
      "The  1300 th epoch : Loss in Train:  4.6305060386657715\n",
      "The  1320 th epoch : Loss in Train:  4.629463195800781\n",
      "The  1340 th epoch : Loss in Train:  4.628401279449463\n",
      "The  1360 th epoch : Loss in Train:  4.627348899841309\n",
      "The  1380 th epoch : Loss in Train:  4.626302719116211\n",
      "The  1400 th epoch : Loss in Train:  4.625260829925537\n",
      "The  1420 th epoch : Loss in Train:  4.624207496643066\n",
      "The  1440 th epoch : Loss in Train:  4.6231513023376465\n",
      "The  1460 th epoch : Loss in Train:  4.6221160888671875\n",
      "The  1480 th epoch : Loss in Train:  4.621057510375977\n",
      "The  1500 th epoch : Loss in Train:  4.6200103759765625\n",
      "The  1520 th epoch : Loss in Train:  4.618969440460205\n",
      "The  1540 th epoch : Loss in Train:  4.617923736572266\n",
      "The  1560 th epoch : Loss in Train:  4.61687707901001\n",
      "The  1580 th epoch : Loss in Train:  4.615825653076172\n",
      "The  1600 th epoch : Loss in Train:  4.6147871017456055\n",
      "The  1620 th epoch : Loss in Train:  4.613742828369141\n",
      "The  1640 th epoch : Loss in Train:  4.612698554992676\n",
      "The  1660 th epoch : Loss in Train:  4.611657619476318\n",
      "The  1680 th epoch : Loss in Train:  4.61061429977417\n",
      "The  1700 th epoch : Loss in Train:  4.609572887420654\n",
      "The  1720 th epoch : Loss in Train:  4.608530521392822\n",
      "The  1740 th epoch : Loss in Train:  4.607491970062256\n",
      "The  1760 th epoch : Loss in Train:  4.606451511383057\n",
      "The  1780 th epoch : Loss in Train:  4.605411529541016\n",
      "The  1800 th epoch : Loss in Train:  4.604372978210449\n",
      "The  1820 th epoch : Loss in Train:  4.603335380554199\n",
      "The  1840 th epoch : Loss in Train:  4.602297782897949\n",
      "The  1860 th epoch : Loss in Train:  4.601261138916016\n",
      "The  1880 th epoch : Loss in Train:  4.600224494934082\n",
      "The  1900 th epoch : Loss in Train:  4.599188804626465\n",
      "The  1920 th epoch : Loss in Train:  4.598153591156006\n",
      "The  1940 th epoch : Loss in Train:  4.597119331359863\n",
      "The  1960 th epoch : Loss in Train:  4.596085548400879\n",
      "The  1980 th epoch : Loss in Train:  4.5950517654418945\n",
      "The  2000 th epoch : Loss in Train:  4.594019889831543\n",
      "The  2020 th epoch : Loss in Train:  4.592987537384033\n",
      "The  2040 th epoch : Loss in Train:  4.591955661773682\n",
      "The  2060 th epoch : Loss in Train:  4.590925216674805\n",
      "The  2080 th epoch : Loss in Train:  4.589895248413086\n",
      "The  2100 th epoch : Loss in Train:  4.588865756988525\n",
      "The  2120 th epoch : Loss in Train:  4.587835788726807\n",
      "The  2140 th epoch : Loss in Train:  4.586807727813721\n",
      "The  2160 th epoch : Loss in Train:  4.585780143737793\n",
      "The  2180 th epoch : Loss in Train:  4.584752559661865\n",
      "The  2200 th epoch : Loss in Train:  4.583725929260254\n",
      "The  2220 th epoch : Loss in Train:  4.582700252532959\n",
      "The  2240 th epoch : Loss in Train:  4.581674098968506\n",
      "The  2260 th epoch : Loss in Train:  4.580648899078369\n",
      "The  2280 th epoch : Loss in Train:  4.579625129699707\n",
      "The  2300 th epoch : Loss in Train:  4.578601360321045\n",
      "The  2320 th epoch : Loss in Train:  4.577579021453857\n",
      "The  2340 th epoch : Loss in Train:  4.5765557289123535\n",
      "The  2360 th epoch : Loss in Train:  4.575534343719482\n",
      "The  2380 th epoch : Loss in Train:  4.574512481689453\n",
      "The  2400 th epoch : Loss in Train:  4.573492050170898\n",
      "The  2420 th epoch : Loss in Train:  4.572471618652344\n",
      "The  2440 th epoch : Loss in Train:  4.571451663970947\n",
      "The  2460 th epoch : Loss in Train:  4.570433139801025\n",
      "The  2480 th epoch : Loss in Train:  4.5694146156311035\n",
      "The  2500 th epoch : Loss in Train:  4.568397045135498\n",
      "The  2520 th epoch : Loss in Train:  4.567379474639893\n",
      "The  2540 th epoch : Loss in Train:  4.5663628578186035\n",
      "The  2560 th epoch : Loss in Train:  4.565346717834473\n",
      "The  2580 th epoch : Loss in Train:  4.5643310546875\n",
      "The  2600 th epoch : Loss in Train:  4.5633158683776855\n",
      "The  2620 th epoch : Loss in Train:  4.562301158905029\n",
      "The  2640 th epoch : Loss in Train:  4.561287879943848\n",
      "The  2660 th epoch : Loss in Train:  4.560274124145508\n",
      "The  2680 th epoch : Loss in Train:  4.559261798858643\n",
      "The  2700 th epoch : Loss in Train:  4.558249473571777\n",
      "The  2720 th epoch : Loss in Train:  4.55723762512207\n",
      "The  2740 th epoch : Loss in Train:  4.556227207183838\n",
      "The  2760 th epoch : Loss in Train:  4.5552167892456055\n",
      "The  2780 th epoch : Loss in Train:  4.554206371307373\n",
      "The  2800 th epoch : Loss in Train:  4.553197383880615\n",
      "The  2820 th epoch : Loss in Train:  4.552188873291016\n",
      "The  2840 th epoch : Loss in Train:  4.551180362701416\n",
      "The  2860 th epoch : Loss in Train:  4.550172328948975\n",
      "The  2880 th epoch : Loss in Train:  4.54916524887085\n",
      "The  2900 th epoch : Loss in Train:  4.548158645629883\n",
      "The  2920 th epoch : Loss in Train:  4.547153472900391\n",
      "The  2940 th epoch : Loss in Train:  4.54614782333374\n",
      "The  2960 th epoch : Loss in Train:  4.545142650604248\n",
      "The  2980 th epoch : Loss in Train:  4.544138431549072\n",
      "The  3000 th epoch : Loss in Train:  4.543135166168213\n",
      "The  3020 th epoch : Loss in Train:  4.542132377624512\n",
      "The  3040 th epoch : Loss in Train:  4.5411295890808105\n",
      "The  3060 th epoch : Loss in Train:  4.540127754211426\n",
      "The  3080 th epoch : Loss in Train:  4.539126396179199\n",
      "The  3100 th epoch : Loss in Train:  4.538125038146973\n",
      "The  3120 th epoch : Loss in Train:  4.5371246337890625\n",
      "The  3140 th epoch : Loss in Train:  4.5361247062683105\n",
      "The  3160 th epoch : Loss in Train:  4.535125732421875\n",
      "The  3180 th epoch : Loss in Train:  4.534127235412598\n",
      "The  3200 th epoch : Loss in Train:  4.533128261566162\n",
      "The  3220 th epoch : Loss in Train:  4.532131195068359\n",
      "The  3240 th epoch : Loss in Train:  4.531134128570557\n",
      "The  3260 th epoch : Loss in Train:  4.530137538909912\n",
      "The  3280 th epoch : Loss in Train:  4.529141902923584\n",
      "The  3300 th epoch : Loss in Train:  4.528146266937256\n",
      "The  3320 th epoch : Loss in Train:  4.527151584625244\n",
      "The  3340 th epoch : Loss in Train:  4.526157379150391\n",
      "The  3360 th epoch : Loss in Train:  4.525163173675537\n",
      "The  3380 th epoch : Loss in Train:  4.524170398712158\n",
      "The  3400 th epoch : Loss in Train:  4.523177146911621\n",
      "The  3420 th epoch : Loss in Train:  4.522185802459717\n",
      "The  3440 th epoch : Loss in Train:  4.521193981170654\n",
      "The  3460 th epoch : Loss in Train:  4.52020263671875\n",
      "The  3480 th epoch : Loss in Train:  4.519212245941162\n",
      "The  3500 th epoch : Loss in Train:  4.518222808837891\n",
      "The  3520 th epoch : Loss in Train:  4.517232894897461\n",
      "The  3540 th epoch : Loss in Train:  4.516244411468506\n",
      "The  3560 th epoch : Loss in Train:  4.515256404876709\n",
      "The  3580 th epoch : Loss in Train:  4.514268398284912\n",
      "The  3600 th epoch : Loss in Train:  4.513280868530273\n",
      "The  3620 th epoch : Loss in Train:  4.512294292449951\n",
      "The  3640 th epoch : Loss in Train:  4.511307716369629\n",
      "The  3660 th epoch : Loss in Train:  4.510322093963623\n",
      "The  3680 th epoch : Loss in Train:  4.509337902069092\n",
      "The  3700 th epoch : Loss in Train:  4.508352756500244\n",
      "The  3720 th epoch : Loss in Train:  4.507369041442871\n",
      "The  3740 th epoch : Loss in Train:  4.506385326385498\n",
      "The  3760 th epoch : Loss in Train:  4.505402565002441\n",
      "The  3780 th epoch : Loss in Train:  4.504420757293701\n",
      "The  3800 th epoch : Loss in Train:  4.5034379959106445\n",
      "The  3820 th epoch : Loss in Train:  4.5024566650390625\n",
      "The  3840 th epoch : Loss in Train:  4.501476287841797\n",
      "The  3860 th epoch : Loss in Train:  4.5004963874816895\n",
      "The  3880 th epoch : Loss in Train:  4.499516010284424\n",
      "The  3900 th epoch : Loss in Train:  4.498536586761475\n",
      "The  3920 th epoch : Loss in Train:  4.49755859375\n",
      "The  3940 th epoch : Loss in Train:  4.496580600738525\n",
      "The  3960 th epoch : Loss in Train:  4.495602130889893\n",
      "The  3980 th epoch : Loss in Train:  4.494625568389893\n",
      "The  4000 th epoch : Loss in Train:  4.493649005889893\n",
      "The  4020 th epoch : Loss in Train:  4.492673873901367\n",
      "The  4040 th epoch : Loss in Train:  4.491697788238525\n",
      "The  4060 th epoch : Loss in Train:  4.49072265625\n",
      "The  4080 th epoch : Loss in Train:  4.489748477935791\n",
      "The  4100 th epoch : Loss in Train:  4.48877477645874\n",
      "The  4120 th epoch : Loss in Train:  4.487802028656006\n",
      "The  4140 th epoch : Loss in Train:  4.4868292808532715\n",
      "The  4160 th epoch : Loss in Train:  4.485856533050537\n",
      "The  4180 th epoch : Loss in Train:  4.484884738922119\n",
      "The  4200 th epoch : Loss in Train:  4.483913898468018\n",
      "The  4220 th epoch : Loss in Train:  4.482943058013916\n",
      "The  4240 th epoch : Loss in Train:  4.481973171234131\n",
      "The  4260 th epoch : Loss in Train:  4.481003284454346\n",
      "The  4280 th epoch : Loss in Train:  4.480034351348877\n",
      "The  4300 th epoch : Loss in Train:  4.47906494140625\n",
      "The  4320 th epoch : Loss in Train:  4.478097915649414\n",
      "The  4340 th epoch : Loss in Train:  4.47713041305542\n",
      "The  4360 th epoch : Loss in Train:  4.476163387298584\n",
      "The  4380 th epoch : Loss in Train:  4.4751973152160645\n",
      "The  4400 th epoch : Loss in Train:  4.474231243133545\n",
      "The  4420 th epoch : Loss in Train:  4.473265647888184\n",
      "The  4440 th epoch : Loss in Train:  4.472301006317139\n",
      "The  4460 th epoch : Loss in Train:  4.471336841583252\n",
      "The  4480 th epoch : Loss in Train:  4.470373153686523\n",
      "The  4500 th epoch : Loss in Train:  4.469409942626953\n",
      "The  4520 th epoch : Loss in Train:  4.468447208404541\n",
      "The  4540 th epoch : Loss in Train:  4.467484951019287\n",
      "The  4560 th epoch : Loss in Train:  4.466523170471191\n",
      "The  4580 th epoch : Loss in Train:  4.465562343597412\n",
      "The  4600 th epoch : Loss in Train:  4.464601039886475\n",
      "The  4620 th epoch : Loss in Train:  4.463641166687012\n",
      "The  4640 th epoch : Loss in Train:  4.462681770324707\n",
      "The  4660 th epoch : Loss in Train:  4.4617228507995605\n",
      "The  4680 th epoch : Loss in Train:  4.460764408111572\n",
      "The  4700 th epoch : Loss in Train:  4.459806442260742\n",
      "The  4720 th epoch : Loss in Train:  4.45884895324707\n",
      "The  4740 th epoch : Loss in Train:  4.457891941070557\n",
      "The  4760 th epoch : Loss in Train:  4.456935405731201\n",
      "The  4780 th epoch : Loss in Train:  4.455979824066162\n",
      "The  4800 th epoch : Loss in Train:  4.455024242401123\n",
      "The  4820 th epoch : Loss in Train:  4.4540696144104\n",
      "The  4840 th epoch : Loss in Train:  4.453115463256836\n",
      "The  4860 th epoch : Loss in Train:  4.4521613121032715\n",
      "The  4880 th epoch : Loss in Train:  4.451208114624023\n",
      "The  4900 th epoch : Loss in Train:  4.450255393981934\n",
      "The  4920 th epoch : Loss in Train:  4.449304103851318\n",
      "The  4940 th epoch : Loss in Train:  4.4483513832092285\n",
      "The  4960 th epoch : Loss in Train:  4.4474005699157715\n",
      "The  4980 th epoch : Loss in Train:  4.446450233459473\n",
      "The  5000 th epoch : Loss in Train:  4.445499897003174\n",
      "the lamda is:  0.1 , the learning rate is:  7.5e-05 .\n",
      "the Loss in Train data is: 4.445499897003174  Loss in Validation is:  2.503737211227417\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.695645332336426\n",
      "The  20 th epoch : Loss in Train:  4.62770938873291\n",
      "The  40 th epoch : Loss in Train:  4.343040466308594\n",
      "The  60 th epoch : Loss in Train:  4.177371978759766\n",
      "The  80 th epoch : Loss in Train:  4.108730792999268\n",
      "The  100 th epoch : Loss in Train:  4.064746856689453\n",
      "The  120 th epoch : Loss in Train:  3.9706785678863525\n",
      "The  140 th epoch : Loss in Train:  3.9895148277282715\n",
      "The  160 th epoch : Loss in Train:  3.863180160522461\n",
      "The  180 th epoch : Loss in Train:  3.8869099617004395\n",
      "The  200 th epoch : Loss in Train:  3.830620288848877\n",
      "The  220 th epoch : Loss in Train:  3.897263288497925\n",
      "The  240 th epoch : Loss in Train:  3.870455265045166\n",
      "The  260 th epoch : Loss in Train:  3.8323211669921875\n",
      "The  280 th epoch : Loss in Train:  3.8396570682525635\n",
      "The  300 th epoch : Loss in Train:  3.7375881671905518\n",
      "The  320 th epoch : Loss in Train:  3.7065627574920654\n",
      "The  340 th epoch : Loss in Train:  3.7539420127868652\n",
      "The  360 th epoch : Loss in Train:  3.6934337615966797\n",
      "The  380 th epoch : Loss in Train:  3.9342546463012695\n",
      "The  400 th epoch : Loss in Train:  3.6721608638763428\n",
      "The  420 th epoch : Loss in Train:  3.854856014251709\n",
      "The  440 th epoch : Loss in Train:  3.768897294998169\n",
      "The  460 th epoch : Loss in Train:  3.748650074005127\n",
      "The  480 th epoch : Loss in Train:  3.714104413986206\n",
      "The  500 th epoch : Loss in Train:  3.6051037311553955\n",
      "The  520 th epoch : Loss in Train:  3.6219301223754883\n",
      "The  540 th epoch : Loss in Train:  3.6498401165008545\n",
      "The  560 th epoch : Loss in Train:  3.6130950450897217\n",
      "The  580 th epoch : Loss in Train:  3.579604387283325\n",
      "The  600 th epoch : Loss in Train:  3.6308252811431885\n",
      "The  620 th epoch : Loss in Train:  3.6107876300811768\n",
      "The  640 th epoch : Loss in Train:  3.5944108963012695\n",
      "The  660 th epoch : Loss in Train:  3.545020341873169\n",
      "The  680 th epoch : Loss in Train:  3.5087685585021973\n",
      "The  700 th epoch : Loss in Train:  3.5161943435668945\n",
      "The  720 th epoch : Loss in Train:  3.582119941711426\n",
      "The  740 th epoch : Loss in Train:  3.4290812015533447\n",
      "The  760 th epoch : Loss in Train:  3.6011240482330322\n",
      "The  780 th epoch : Loss in Train:  3.4830679893493652\n",
      "The  800 th epoch : Loss in Train:  3.492556571960449\n",
      "The  820 th epoch : Loss in Train:  3.5320980548858643\n",
      "The  840 th epoch : Loss in Train:  3.5751430988311768\n",
      "The  860 th epoch : Loss in Train:  3.5212831497192383\n",
      "The  880 th epoch : Loss in Train:  3.5604259967803955\n",
      "The  900 th epoch : Loss in Train:  3.4873015880584717\n",
      "The  920 th epoch : Loss in Train:  3.5009806156158447\n",
      "The  940 th epoch : Loss in Train:  3.6147878170013428\n",
      "The  960 th epoch : Loss in Train:  3.419975996017456\n",
      "The  980 th epoch : Loss in Train:  3.445754289627075\n",
      "The  1000 th epoch : Loss in Train:  3.4807209968566895\n",
      "The  1020 th epoch : Loss in Train:  3.429189920425415\n",
      "The  1040 th epoch : Loss in Train:  3.5271530151367188\n",
      "The  1060 th epoch : Loss in Train:  3.3958098888397217\n",
      "The  1080 th epoch : Loss in Train:  3.4592080116271973\n",
      "The  1100 th epoch : Loss in Train:  3.562917709350586\n",
      "The  1120 th epoch : Loss in Train:  3.4631435871124268\n",
      "The  1140 th epoch : Loss in Train:  3.528327465057373\n",
      "The  1160 th epoch : Loss in Train:  3.391150712966919\n",
      "The  1180 th epoch : Loss in Train:  3.4008994102478027\n",
      "The  1200 th epoch : Loss in Train:  3.3950812816619873\n",
      "The  1220 th epoch : Loss in Train:  3.4048008918762207\n",
      "The  1240 th epoch : Loss in Train:  3.4655182361602783\n",
      "The  1260 th epoch : Loss in Train:  3.377664566040039\n",
      "The  1280 th epoch : Loss in Train:  3.3515520095825195\n",
      "The  1300 th epoch : Loss in Train:  3.420820474624634\n",
      "The  1320 th epoch : Loss in Train:  3.4794254302978516\n",
      "The  1340 th epoch : Loss in Train:  3.4585816860198975\n",
      "The  1360 th epoch : Loss in Train:  3.3430795669555664\n",
      "The  1380 th epoch : Loss in Train:  3.3091654777526855\n",
      "The  1400 th epoch : Loss in Train:  3.3819944858551025\n",
      "The  1420 th epoch : Loss in Train:  3.3877620697021484\n",
      "The  1440 th epoch : Loss in Train:  3.354574203491211\n",
      "The  1460 th epoch : Loss in Train:  3.3830413818359375\n",
      "The  1480 th epoch : Loss in Train:  3.3288512229919434\n",
      "The  1500 th epoch : Loss in Train:  3.313810110092163\n",
      "The  1520 th epoch : Loss in Train:  3.3843696117401123\n",
      "The  1540 th epoch : Loss in Train:  3.302386522293091\n",
      "The  1560 th epoch : Loss in Train:  3.3716089725494385\n",
      "The  1580 th epoch : Loss in Train:  3.3905375003814697\n",
      "The  1600 th epoch : Loss in Train:  3.3086440563201904\n",
      "The  1620 th epoch : Loss in Train:  3.3575937747955322\n",
      "The  1640 th epoch : Loss in Train:  3.359748125076294\n",
      "The  1660 th epoch : Loss in Train:  3.380887031555176\n",
      "The  1680 th epoch : Loss in Train:  3.194706439971924\n",
      "The  1700 th epoch : Loss in Train:  3.360031843185425\n",
      "The  1720 th epoch : Loss in Train:  3.3549365997314453\n",
      "The  1740 th epoch : Loss in Train:  3.4167656898498535\n",
      "The  1760 th epoch : Loss in Train:  3.417471170425415\n",
      "The  1780 th epoch : Loss in Train:  3.3853957653045654\n",
      "The  1800 th epoch : Loss in Train:  3.3078958988189697\n",
      "The  1820 th epoch : Loss in Train:  3.234593152999878\n",
      "The  1840 th epoch : Loss in Train:  3.3654918670654297\n",
      "The  1860 th epoch : Loss in Train:  3.323505163192749\n",
      "The  1880 th epoch : Loss in Train:  3.2530829906463623\n",
      "The  1900 th epoch : Loss in Train:  3.4110524654388428\n",
      "The  1920 th epoch : Loss in Train:  3.3005545139312744\n",
      "The  1940 th epoch : Loss in Train:  3.3139419555664062\n",
      "The  1960 th epoch : Loss in Train:  3.2703232765197754\n",
      "The  1980 th epoch : Loss in Train:  3.320512533187866\n",
      "The  2000 th epoch : Loss in Train:  3.2900583744049072\n",
      "The  2020 th epoch : Loss in Train:  3.2901763916015625\n",
      "The  2040 th epoch : Loss in Train:  3.354098081588745\n",
      "The  2060 th epoch : Loss in Train:  3.3255600929260254\n",
      "The  2080 th epoch : Loss in Train:  3.3788363933563232\n",
      "The  2100 th epoch : Loss in Train:  3.330214262008667\n",
      "The  2120 th epoch : Loss in Train:  3.313491106033325\n",
      "The  2140 th epoch : Loss in Train:  3.232673168182373\n",
      "The  2160 th epoch : Loss in Train:  3.344606637954712\n",
      "The  2180 th epoch : Loss in Train:  3.213263750076294\n",
      "The  2200 th epoch : Loss in Train:  3.2038633823394775\n",
      "The  2220 th epoch : Loss in Train:  3.1715614795684814\n",
      "The  2240 th epoch : Loss in Train:  3.3101658821105957\n",
      "The  2260 th epoch : Loss in Train:  3.228370189666748\n",
      "The  2280 th epoch : Loss in Train:  3.2363054752349854\n",
      "The  2300 th epoch : Loss in Train:  3.3538622856140137\n",
      "The  2320 th epoch : Loss in Train:  3.2820472717285156\n",
      "The  2340 th epoch : Loss in Train:  3.2497048377990723\n",
      "The  2360 th epoch : Loss in Train:  3.2213830947875977\n",
      "The  2380 th epoch : Loss in Train:  3.3155834674835205\n",
      "The  2400 th epoch : Loss in Train:  3.215414047241211\n",
      "The  2420 th epoch : Loss in Train:  3.1934638023376465\n",
      "The  2440 th epoch : Loss in Train:  3.3937911987304688\n",
      "The  2460 th epoch : Loss in Train:  3.18575119972229\n",
      "The  2480 th epoch : Loss in Train:  3.2244713306427\n",
      "The  2500 th epoch : Loss in Train:  3.1858367919921875\n",
      "The  2520 th epoch : Loss in Train:  3.1714425086975098\n",
      "The  2540 th epoch : Loss in Train:  3.17508864402771\n",
      "The  2560 th epoch : Loss in Train:  3.273653507232666\n",
      "The  2580 th epoch : Loss in Train:  3.1720590591430664\n",
      "The  2600 th epoch : Loss in Train:  3.149446964263916\n",
      "The  2620 th epoch : Loss in Train:  3.305394172668457\n",
      "The  2640 th epoch : Loss in Train:  3.131915330886841\n",
      "The  2660 th epoch : Loss in Train:  3.2585177421569824\n",
      "The  2680 th epoch : Loss in Train:  3.1410512924194336\n",
      "The  2700 th epoch : Loss in Train:  3.2313272953033447\n",
      "The  2720 th epoch : Loss in Train:  3.298393964767456\n",
      "The  2740 th epoch : Loss in Train:  3.232778787612915\n",
      "The  2760 th epoch : Loss in Train:  3.107964277267456\n",
      "The  2780 th epoch : Loss in Train:  3.279909133911133\n",
      "The  2800 th epoch : Loss in Train:  3.3317315578460693\n",
      "The  2820 th epoch : Loss in Train:  3.275087356567383\n",
      "The  2840 th epoch : Loss in Train:  3.352658271789551\n",
      "The  2860 th epoch : Loss in Train:  3.1132678985595703\n",
      "The  2880 th epoch : Loss in Train:  3.2352497577667236\n",
      "The  2900 th epoch : Loss in Train:  3.1781952381134033\n",
      "The  2920 th epoch : Loss in Train:  3.24859619140625\n",
      "The  2940 th epoch : Loss in Train:  3.33060359954834\n",
      "The  2960 th epoch : Loss in Train:  3.1890079975128174\n",
      "The  2980 th epoch : Loss in Train:  3.2396347522735596\n",
      "The  3000 th epoch : Loss in Train:  3.2601232528686523\n",
      "The  3020 th epoch : Loss in Train:  3.209099054336548\n",
      "The  3040 th epoch : Loss in Train:  3.3005483150482178\n",
      "The  3060 th epoch : Loss in Train:  3.1400256156921387\n",
      "The  3080 th epoch : Loss in Train:  3.1764659881591797\n",
      "The  3100 th epoch : Loss in Train:  3.2202024459838867\n",
      "The  3120 th epoch : Loss in Train:  3.155818223953247\n",
      "The  3140 th epoch : Loss in Train:  3.135993003845215\n",
      "The  3160 th epoch : Loss in Train:  3.1704206466674805\n",
      "The  3180 th epoch : Loss in Train:  3.278233289718628\n",
      "The  3200 th epoch : Loss in Train:  3.287003755569458\n",
      "The  3220 th epoch : Loss in Train:  3.186570644378662\n",
      "The  3240 th epoch : Loss in Train:  3.105151414871216\n",
      "The  3260 th epoch : Loss in Train:  3.221404552459717\n",
      "The  3280 th epoch : Loss in Train:  3.247723340988159\n",
      "The  3300 th epoch : Loss in Train:  3.228895902633667\n",
      "The  3320 th epoch : Loss in Train:  3.108907699584961\n",
      "The  3340 th epoch : Loss in Train:  3.085111141204834\n",
      "The  3360 th epoch : Loss in Train:  3.1129651069641113\n",
      "The  3380 th epoch : Loss in Train:  3.13791561126709\n",
      "The  3400 th epoch : Loss in Train:  3.154764175415039\n",
      "The  3420 th epoch : Loss in Train:  3.2522671222686768\n",
      "The  3440 th epoch : Loss in Train:  3.175039768218994\n",
      "The  3460 th epoch : Loss in Train:  3.1806252002716064\n",
      "The  3480 th epoch : Loss in Train:  3.104527235031128\n",
      "The  3500 th epoch : Loss in Train:  3.168471097946167\n",
      "The  3520 th epoch : Loss in Train:  3.3250067234039307\n",
      "The  3540 th epoch : Loss in Train:  3.371626615524292\n",
      "The  3560 th epoch : Loss in Train:  3.113466262817383\n",
      "The  3580 th epoch : Loss in Train:  3.2122113704681396\n",
      "The  3600 th epoch : Loss in Train:  3.3231983184814453\n",
      "The  3620 th epoch : Loss in Train:  3.2612924575805664\n",
      "The  3640 th epoch : Loss in Train:  3.223095655441284\n",
      "The  3660 th epoch : Loss in Train:  3.166494131088257\n",
      "The  3680 th epoch : Loss in Train:  3.1569602489471436\n",
      "The  3700 th epoch : Loss in Train:  3.1093075275421143\n",
      "The  3720 th epoch : Loss in Train:  3.2961578369140625\n",
      "The  3740 th epoch : Loss in Train:  3.2933266162872314\n",
      "The  3760 th epoch : Loss in Train:  3.2658097743988037\n",
      "The  3780 th epoch : Loss in Train:  3.189201593399048\n",
      "The  3800 th epoch : Loss in Train:  3.179853916168213\n",
      "The  3820 th epoch : Loss in Train:  3.084564208984375\n",
      "The  3840 th epoch : Loss in Train:  3.190272331237793\n",
      "The  3860 th epoch : Loss in Train:  3.3129775524139404\n",
      "The  3880 th epoch : Loss in Train:  3.247159719467163\n",
      "The  3900 th epoch : Loss in Train:  3.1026933193206787\n",
      "The  3920 th epoch : Loss in Train:  3.099738121032715\n",
      "The  3940 th epoch : Loss in Train:  3.2767980098724365\n",
      "The  3960 th epoch : Loss in Train:  3.208710193634033\n",
      "The  3980 th epoch : Loss in Train:  3.1439292430877686\n",
      "The  4000 th epoch : Loss in Train:  3.1499900817871094\n",
      "The  4020 th epoch : Loss in Train:  3.28830885887146\n",
      "The  4040 th epoch : Loss in Train:  3.2326598167419434\n",
      "The  4060 th epoch : Loss in Train:  3.0999977588653564\n",
      "The  4080 th epoch : Loss in Train:  3.286442279815674\n",
      "The  4100 th epoch : Loss in Train:  3.2377660274505615\n",
      "The  4120 th epoch : Loss in Train:  3.0789434909820557\n",
      "The  4140 th epoch : Loss in Train:  3.0819156169891357\n",
      "The  4160 th epoch : Loss in Train:  3.162870407104492\n",
      "The  4180 th epoch : Loss in Train:  3.212686777114868\n",
      "The  4200 th epoch : Loss in Train:  3.2722601890563965\n",
      "The  4220 th epoch : Loss in Train:  3.1096205711364746\n",
      "The  4240 th epoch : Loss in Train:  3.099630355834961\n",
      "The  4260 th epoch : Loss in Train:  3.2658050060272217\n",
      "The  4280 th epoch : Loss in Train:  3.2732534408569336\n",
      "The  4300 th epoch : Loss in Train:  3.123335599899292\n",
      "The  4320 th epoch : Loss in Train:  3.2533702850341797\n",
      "The  4340 th epoch : Loss in Train:  3.1283421516418457\n",
      "The  4360 th epoch : Loss in Train:  3.0982272624969482\n",
      "The  4380 th epoch : Loss in Train:  3.1374475955963135\n",
      "The  4400 th epoch : Loss in Train:  3.225041151046753\n",
      "The  4420 th epoch : Loss in Train:  3.1659979820251465\n",
      "The  4440 th epoch : Loss in Train:  3.325788974761963\n",
      "The  4460 th epoch : Loss in Train:  3.1054203510284424\n",
      "The  4480 th epoch : Loss in Train:  3.144446849822998\n",
      "The  4500 th epoch : Loss in Train:  3.1203556060791016\n",
      "The  4520 th epoch : Loss in Train:  3.407360315322876\n",
      "The  4540 th epoch : Loss in Train:  3.146423816680908\n",
      "The  4560 th epoch : Loss in Train:  3.3608779907226562\n",
      "The  4580 th epoch : Loss in Train:  3.1139795780181885\n",
      "The  4600 th epoch : Loss in Train:  3.1651694774627686\n",
      "The  4620 th epoch : Loss in Train:  3.20222806930542\n",
      "The  4640 th epoch : Loss in Train:  3.111901044845581\n",
      "The  4660 th epoch : Loss in Train:  3.272703170776367\n",
      "The  4680 th epoch : Loss in Train:  3.1404592990875244\n",
      "The  4700 th epoch : Loss in Train:  3.0994417667388916\n",
      "The  4720 th epoch : Loss in Train:  3.155000925064087\n",
      "The  4740 th epoch : Loss in Train:  3.122887372970581\n",
      "The  4760 th epoch : Loss in Train:  3.073617696762085\n",
      "The  4780 th epoch : Loss in Train:  3.287806749343872\n",
      "The  4800 th epoch : Loss in Train:  3.185711622238159\n",
      "The  4820 th epoch : Loss in Train:  3.3518764972686768\n",
      "The  4840 th epoch : Loss in Train:  3.2346770763397217\n",
      "The  4860 th epoch : Loss in Train:  3.2531814575195312\n",
      "The  4880 th epoch : Loss in Train:  3.131380558013916\n",
      "The  4900 th epoch : Loss in Train:  3.094160318374634\n",
      "The  4920 th epoch : Loss in Train:  3.2243826389312744\n",
      "The  4940 th epoch : Loss in Train:  3.081585645675659\n",
      "The  4960 th epoch : Loss in Train:  3.128333806991577\n",
      "The  4980 th epoch : Loss in Train:  3.112898111343384\n",
      "The  5000 th epoch : Loss in Train:  3.0787782669067383\n",
      "the lamda is:  0.01 , the learning rate is:  0.003 .\n",
      "the Loss in Train data is: 3.0787782669067383  Loss in Validation is:  2.548336982727051\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.698200702667236\n",
      "The  20 th epoch : Loss in Train:  4.681748867034912\n",
      "The  40 th epoch : Loss in Train:  4.662895202636719\n",
      "The  60 th epoch : Loss in Train:  4.567406177520752\n",
      "The  80 th epoch : Loss in Train:  4.396003246307373\n",
      "The  100 th epoch : Loss in Train:  4.326947212219238\n",
      "The  120 th epoch : Loss in Train:  4.282118797302246\n",
      "The  140 th epoch : Loss in Train:  4.177641868591309\n",
      "The  160 th epoch : Loss in Train:  4.080002784729004\n",
      "The  180 th epoch : Loss in Train:  4.062712669372559\n",
      "The  200 th epoch : Loss in Train:  4.03688383102417\n",
      "The  220 th epoch : Loss in Train:  4.045431613922119\n",
      "The  240 th epoch : Loss in Train:  4.026207447052002\n",
      "The  260 th epoch : Loss in Train:  3.9720988273620605\n",
      "The  280 th epoch : Loss in Train:  3.949275016784668\n",
      "The  300 th epoch : Loss in Train:  3.983541488647461\n",
      "The  320 th epoch : Loss in Train:  4.006500244140625\n",
      "The  340 th epoch : Loss in Train:  3.9900736808776855\n",
      "The  360 th epoch : Loss in Train:  4.067785739898682\n",
      "The  380 th epoch : Loss in Train:  3.9560353755950928\n",
      "The  400 th epoch : Loss in Train:  3.8566508293151855\n",
      "The  420 th epoch : Loss in Train:  3.8371336460113525\n",
      "The  440 th epoch : Loss in Train:  3.880648136138916\n",
      "The  460 th epoch : Loss in Train:  3.8787543773651123\n",
      "The  480 th epoch : Loss in Train:  3.9455952644348145\n",
      "The  500 th epoch : Loss in Train:  3.827110767364502\n",
      "The  520 th epoch : Loss in Train:  3.854433059692383\n",
      "The  540 th epoch : Loss in Train:  3.8445048332214355\n",
      "The  560 th epoch : Loss in Train:  3.8255457878112793\n",
      "The  580 th epoch : Loss in Train:  3.850250720977783\n",
      "The  600 th epoch : Loss in Train:  3.8540308475494385\n",
      "The  620 th epoch : Loss in Train:  3.7904722690582275\n",
      "The  640 th epoch : Loss in Train:  3.7866334915161133\n",
      "The  660 th epoch : Loss in Train:  3.8256969451904297\n",
      "The  680 th epoch : Loss in Train:  3.839963436126709\n",
      "The  700 th epoch : Loss in Train:  3.8147501945495605\n",
      "The  720 th epoch : Loss in Train:  3.8279857635498047\n",
      "The  740 th epoch : Loss in Train:  3.8497908115386963\n",
      "The  760 th epoch : Loss in Train:  3.8775875568389893\n",
      "The  780 th epoch : Loss in Train:  3.9279091358184814\n",
      "The  800 th epoch : Loss in Train:  3.874995231628418\n",
      "The  820 th epoch : Loss in Train:  3.7240331172943115\n",
      "The  840 th epoch : Loss in Train:  3.8981339931488037\n",
      "The  860 th epoch : Loss in Train:  3.7318949699401855\n",
      "The  880 th epoch : Loss in Train:  3.7898354530334473\n",
      "The  900 th epoch : Loss in Train:  3.7643349170684814\n",
      "The  920 th epoch : Loss in Train:  3.7159078121185303\n",
      "The  940 th epoch : Loss in Train:  3.7439944744110107\n",
      "The  960 th epoch : Loss in Train:  3.734837293624878\n",
      "The  980 th epoch : Loss in Train:  3.821729898452759\n",
      "The  1000 th epoch : Loss in Train:  3.7289717197418213\n",
      "The  1020 th epoch : Loss in Train:  3.71199631690979\n",
      "The  1040 th epoch : Loss in Train:  3.7180094718933105\n",
      "The  1060 th epoch : Loss in Train:  3.70619797706604\n",
      "The  1080 th epoch : Loss in Train:  3.7045605182647705\n",
      "The  1100 th epoch : Loss in Train:  3.9768424034118652\n",
      "The  1120 th epoch : Loss in Train:  3.8509151935577393\n",
      "The  1140 th epoch : Loss in Train:  3.7469570636749268\n",
      "The  1160 th epoch : Loss in Train:  3.658324718475342\n",
      "The  1180 th epoch : Loss in Train:  3.613276720046997\n",
      "The  1200 th epoch : Loss in Train:  3.782076835632324\n",
      "The  1220 th epoch : Loss in Train:  3.679938793182373\n",
      "The  1240 th epoch : Loss in Train:  3.615877151489258\n",
      "The  1260 th epoch : Loss in Train:  3.663724899291992\n",
      "The  1280 th epoch : Loss in Train:  3.6582958698272705\n",
      "The  1300 th epoch : Loss in Train:  3.562542676925659\n",
      "The  1320 th epoch : Loss in Train:  3.605618476867676\n",
      "The  1340 th epoch : Loss in Train:  3.669872522354126\n",
      "The  1360 th epoch : Loss in Train:  3.577204942703247\n",
      "The  1380 th epoch : Loss in Train:  3.6816020011901855\n",
      "The  1400 th epoch : Loss in Train:  3.6463582515716553\n",
      "The  1420 th epoch : Loss in Train:  3.6377310752868652\n",
      "The  1440 th epoch : Loss in Train:  3.665020704269409\n",
      "The  1460 th epoch : Loss in Train:  3.5930027961730957\n",
      "The  1480 th epoch : Loss in Train:  3.7371985912323\n",
      "The  1500 th epoch : Loss in Train:  3.6577212810516357\n",
      "The  1520 th epoch : Loss in Train:  3.6815385818481445\n",
      "The  1540 th epoch : Loss in Train:  3.768550157546997\n",
      "The  1560 th epoch : Loss in Train:  3.5500504970550537\n",
      "The  1580 th epoch : Loss in Train:  3.600085496902466\n",
      "The  1600 th epoch : Loss in Train:  3.7109267711639404\n",
      "The  1620 th epoch : Loss in Train:  3.846372604370117\n",
      "The  1640 th epoch : Loss in Train:  3.584402084350586\n",
      "The  1660 th epoch : Loss in Train:  3.589354991912842\n",
      "The  1680 th epoch : Loss in Train:  3.530134916305542\n",
      "The  1700 th epoch : Loss in Train:  3.662661075592041\n",
      "The  1720 th epoch : Loss in Train:  3.68965220451355\n",
      "The  1740 th epoch : Loss in Train:  3.644291639328003\n",
      "The  1760 th epoch : Loss in Train:  3.6735482215881348\n",
      "The  1780 th epoch : Loss in Train:  3.6320250034332275\n",
      "The  1800 th epoch : Loss in Train:  3.696728467941284\n",
      "The  1820 th epoch : Loss in Train:  3.4863669872283936\n",
      "The  1840 th epoch : Loss in Train:  3.7147159576416016\n",
      "The  1860 th epoch : Loss in Train:  3.5367190837860107\n",
      "The  1880 th epoch : Loss in Train:  3.6795971393585205\n",
      "The  1900 th epoch : Loss in Train:  3.5871422290802\n",
      "The  1920 th epoch : Loss in Train:  3.4537558555603027\n",
      "The  1940 th epoch : Loss in Train:  3.584623336791992\n",
      "The  1960 th epoch : Loss in Train:  3.5854814052581787\n",
      "The  1980 th epoch : Loss in Train:  3.5344161987304688\n",
      "The  2000 th epoch : Loss in Train:  3.5426435470581055\n",
      "The  2020 th epoch : Loss in Train:  3.513028144836426\n",
      "The  2040 th epoch : Loss in Train:  3.452316999435425\n",
      "The  2060 th epoch : Loss in Train:  3.525881767272949\n",
      "The  2080 th epoch : Loss in Train:  3.5472187995910645\n",
      "The  2100 th epoch : Loss in Train:  3.5238349437713623\n",
      "The  2120 th epoch : Loss in Train:  3.6396324634552\n",
      "The  2140 th epoch : Loss in Train:  3.467712879180908\n",
      "The  2160 th epoch : Loss in Train:  3.459702730178833\n",
      "The  2180 th epoch : Loss in Train:  3.5467946529388428\n",
      "The  2200 th epoch : Loss in Train:  3.5017378330230713\n",
      "The  2220 th epoch : Loss in Train:  3.4923622608184814\n",
      "The  2240 th epoch : Loss in Train:  3.501988172531128\n",
      "The  2260 th epoch : Loss in Train:  3.4789533615112305\n",
      "The  2280 th epoch : Loss in Train:  3.560121536254883\n",
      "The  2300 th epoch : Loss in Train:  3.5389885902404785\n",
      "The  2320 th epoch : Loss in Train:  3.4891180992126465\n",
      "The  2340 th epoch : Loss in Train:  3.500383138656616\n",
      "The  2360 th epoch : Loss in Train:  3.4486496448516846\n",
      "The  2380 th epoch : Loss in Train:  3.4789958000183105\n",
      "The  2400 th epoch : Loss in Train:  3.5129058361053467\n",
      "The  2420 th epoch : Loss in Train:  3.4309303760528564\n",
      "The  2440 th epoch : Loss in Train:  3.5381362438201904\n",
      "The  2460 th epoch : Loss in Train:  3.4942827224731445\n",
      "The  2480 th epoch : Loss in Train:  3.5251541137695312\n",
      "The  2500 th epoch : Loss in Train:  3.5465810298919678\n",
      "The  2520 th epoch : Loss in Train:  3.443481922149658\n",
      "The  2540 th epoch : Loss in Train:  3.4605889320373535\n",
      "The  2560 th epoch : Loss in Train:  3.4008331298828125\n",
      "The  2580 th epoch : Loss in Train:  3.3371422290802\n",
      "The  2600 th epoch : Loss in Train:  3.406283378601074\n",
      "The  2620 th epoch : Loss in Train:  3.478590250015259\n",
      "The  2640 th epoch : Loss in Train:  3.508077621459961\n",
      "The  2660 th epoch : Loss in Train:  3.329665422439575\n",
      "The  2680 th epoch : Loss in Train:  3.4334142208099365\n",
      "The  2700 th epoch : Loss in Train:  3.3578851222991943\n",
      "The  2720 th epoch : Loss in Train:  3.4702401161193848\n",
      "The  2740 th epoch : Loss in Train:  3.4066262245178223\n",
      "The  2760 th epoch : Loss in Train:  3.445410966873169\n",
      "The  2780 th epoch : Loss in Train:  3.3874194622039795\n",
      "The  2800 th epoch : Loss in Train:  3.4300453662872314\n",
      "The  2820 th epoch : Loss in Train:  3.4241549968719482\n",
      "The  2840 th epoch : Loss in Train:  3.4563939571380615\n",
      "The  2860 th epoch : Loss in Train:  3.5921730995178223\n",
      "The  2880 th epoch : Loss in Train:  3.345665693283081\n",
      "The  2900 th epoch : Loss in Train:  3.3959062099456787\n",
      "The  2920 th epoch : Loss in Train:  3.4558467864990234\n",
      "The  2940 th epoch : Loss in Train:  3.312096357345581\n",
      "The  2960 th epoch : Loss in Train:  3.4129767417907715\n",
      "The  2980 th epoch : Loss in Train:  3.31901478767395\n",
      "The  3000 th epoch : Loss in Train:  3.4367446899414062\n",
      "The  3020 th epoch : Loss in Train:  3.35860013961792\n",
      "The  3040 th epoch : Loss in Train:  3.427704334259033\n",
      "The  3060 th epoch : Loss in Train:  3.4125514030456543\n",
      "The  3080 th epoch : Loss in Train:  3.3951079845428467\n",
      "The  3100 th epoch : Loss in Train:  3.4917585849761963\n",
      "The  3120 th epoch : Loss in Train:  3.379932165145874\n",
      "The  3140 th epoch : Loss in Train:  3.361750364303589\n",
      "The  3160 th epoch : Loss in Train:  3.359609842300415\n",
      "The  3180 th epoch : Loss in Train:  3.3893439769744873\n",
      "The  3200 th epoch : Loss in Train:  3.3289666175842285\n",
      "The  3220 th epoch : Loss in Train:  3.391530990600586\n",
      "The  3240 th epoch : Loss in Train:  3.4633595943450928\n",
      "The  3260 th epoch : Loss in Train:  3.4733822345733643\n",
      "The  3280 th epoch : Loss in Train:  3.420283555984497\n",
      "The  3300 th epoch : Loss in Train:  3.2905025482177734\n",
      "The  3320 th epoch : Loss in Train:  3.436601400375366\n",
      "The  3340 th epoch : Loss in Train:  3.306816816329956\n",
      "The  3360 th epoch : Loss in Train:  3.3886430263519287\n",
      "The  3380 th epoch : Loss in Train:  3.349353313446045\n",
      "The  3400 th epoch : Loss in Train:  3.3907668590545654\n",
      "The  3420 th epoch : Loss in Train:  3.310708522796631\n",
      "The  3440 th epoch : Loss in Train:  3.3079700469970703\n",
      "The  3460 th epoch : Loss in Train:  3.452420711517334\n",
      "The  3480 th epoch : Loss in Train:  3.3621902465820312\n",
      "The  3500 th epoch : Loss in Train:  3.282153367996216\n",
      "The  3520 th epoch : Loss in Train:  3.4809906482696533\n",
      "The  3540 th epoch : Loss in Train:  3.3382205963134766\n",
      "The  3560 th epoch : Loss in Train:  3.3052961826324463\n",
      "The  3580 th epoch : Loss in Train:  3.3660569190979004\n",
      "The  3600 th epoch : Loss in Train:  3.248481273651123\n",
      "The  3620 th epoch : Loss in Train:  3.2211318016052246\n",
      "The  3640 th epoch : Loss in Train:  3.4658408164978027\n",
      "The  3660 th epoch : Loss in Train:  3.3995449542999268\n",
      "The  3680 th epoch : Loss in Train:  3.404947280883789\n",
      "The  3700 th epoch : Loss in Train:  3.368359327316284\n",
      "The  3720 th epoch : Loss in Train:  3.2714011669158936\n",
      "The  3740 th epoch : Loss in Train:  3.3993642330169678\n",
      "The  3760 th epoch : Loss in Train:  3.2730014324188232\n",
      "The  3780 th epoch : Loss in Train:  3.401801586151123\n",
      "The  3800 th epoch : Loss in Train:  3.46112060546875\n",
      "The  3820 th epoch : Loss in Train:  3.4747016429901123\n",
      "The  3840 th epoch : Loss in Train:  3.2486202716827393\n",
      "The  3860 th epoch : Loss in Train:  3.3765146732330322\n",
      "The  3880 th epoch : Loss in Train:  3.4295454025268555\n",
      "The  3900 th epoch : Loss in Train:  3.3561489582061768\n",
      "The  3920 th epoch : Loss in Train:  3.309990406036377\n",
      "The  3940 th epoch : Loss in Train:  3.4341671466827393\n",
      "The  3960 th epoch : Loss in Train:  3.351081609725952\n",
      "The  3980 th epoch : Loss in Train:  3.2361812591552734\n",
      "The  4000 th epoch : Loss in Train:  3.283892869949341\n",
      "The  4020 th epoch : Loss in Train:  3.3382434844970703\n",
      "The  4040 th epoch : Loss in Train:  3.3381571769714355\n",
      "The  4060 th epoch : Loss in Train:  3.3539938926696777\n",
      "The  4080 th epoch : Loss in Train:  3.173368453979492\n",
      "The  4100 th epoch : Loss in Train:  3.308619260787964\n",
      "The  4120 th epoch : Loss in Train:  3.1860098838806152\n",
      "The  4140 th epoch : Loss in Train:  3.3887767791748047\n",
      "The  4160 th epoch : Loss in Train:  3.274567127227783\n",
      "The  4180 th epoch : Loss in Train:  3.314607620239258\n",
      "The  4200 th epoch : Loss in Train:  3.222224473953247\n",
      "The  4220 th epoch : Loss in Train:  3.2134106159210205\n",
      "The  4240 th epoch : Loss in Train:  3.284668207168579\n",
      "The  4260 th epoch : Loss in Train:  3.267662763595581\n",
      "The  4280 th epoch : Loss in Train:  3.235445499420166\n",
      "The  4300 th epoch : Loss in Train:  3.3303864002227783\n",
      "The  4320 th epoch : Loss in Train:  3.4930219650268555\n",
      "The  4340 th epoch : Loss in Train:  3.3421244621276855\n",
      "The  4360 th epoch : Loss in Train:  3.360783338546753\n",
      "The  4380 th epoch : Loss in Train:  3.28682017326355\n",
      "The  4400 th epoch : Loss in Train:  3.272719383239746\n",
      "The  4420 th epoch : Loss in Train:  3.1940181255340576\n",
      "The  4440 th epoch : Loss in Train:  3.3811397552490234\n",
      "The  4460 th epoch : Loss in Train:  3.329075336456299\n",
      "The  4480 th epoch : Loss in Train:  3.4535157680511475\n",
      "The  4500 th epoch : Loss in Train:  3.3244712352752686\n",
      "The  4520 th epoch : Loss in Train:  3.325900077819824\n",
      "The  4540 th epoch : Loss in Train:  3.2857768535614014\n",
      "The  4560 th epoch : Loss in Train:  3.2047390937805176\n",
      "The  4580 th epoch : Loss in Train:  3.228567123413086\n",
      "The  4600 th epoch : Loss in Train:  3.3654708862304688\n",
      "The  4620 th epoch : Loss in Train:  3.2806830406188965\n",
      "The  4640 th epoch : Loss in Train:  3.2952921390533447\n",
      "The  4660 th epoch : Loss in Train:  3.2338593006134033\n",
      "The  4680 th epoch : Loss in Train:  3.2655959129333496\n",
      "The  4700 th epoch : Loss in Train:  3.3638193607330322\n",
      "The  4720 th epoch : Loss in Train:  3.261911630630493\n",
      "The  4740 th epoch : Loss in Train:  3.174307346343994\n",
      "The  4760 th epoch : Loss in Train:  3.3055059909820557\n",
      "The  4780 th epoch : Loss in Train:  3.3518385887145996\n",
      "The  4800 th epoch : Loss in Train:  3.337128162384033\n",
      "The  4820 th epoch : Loss in Train:  3.362476110458374\n",
      "The  4840 th epoch : Loss in Train:  3.232135057449341\n",
      "The  4860 th epoch : Loss in Train:  3.2318115234375\n",
      "The  4880 th epoch : Loss in Train:  3.2277514934539795\n",
      "The  4900 th epoch : Loss in Train:  3.376255989074707\n",
      "The  4920 th epoch : Loss in Train:  3.286482810974121\n",
      "The  4940 th epoch : Loss in Train:  3.285322904586792\n",
      "The  4960 th epoch : Loss in Train:  3.3318374156951904\n",
      "The  4980 th epoch : Loss in Train:  3.290414810180664\n",
      "The  5000 th epoch : Loss in Train:  3.1410515308380127\n",
      "the lamda is:  0.01 , the learning rate is:  0.001 .\n",
      "the Loss in Train data is: 3.1410515308380127  Loss in Validation is:  2.631136178970337\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.700445652008057\n",
      "The  20 th epoch : Loss in Train:  4.6989426612854\n",
      "The  40 th epoch : Loss in Train:  4.697423934936523\n",
      "The  60 th epoch : Loss in Train:  4.696086883544922\n",
      "The  80 th epoch : Loss in Train:  4.694499492645264\n",
      "The  100 th epoch : Loss in Train:  4.693016529083252\n",
      "The  120 th epoch : Loss in Train:  4.691553115844727\n",
      "The  140 th epoch : Loss in Train:  4.689965724945068\n",
      "The  160 th epoch : Loss in Train:  4.688398838043213\n",
      "The  180 th epoch : Loss in Train:  4.687017917633057\n",
      "The  200 th epoch : Loss in Train:  4.685394287109375\n",
      "The  220 th epoch : Loss in Train:  4.683950901031494\n",
      "The  240 th epoch : Loss in Train:  4.681833744049072\n",
      "The  260 th epoch : Loss in Train:  4.680351257324219\n",
      "The  280 th epoch : Loss in Train:  4.679370880126953\n",
      "The  300 th epoch : Loss in Train:  4.675590991973877\n",
      "The  320 th epoch : Loss in Train:  4.673568248748779\n",
      "The  340 th epoch : Loss in Train:  4.669001579284668\n",
      "The  360 th epoch : Loss in Train:  4.6707682609558105\n",
      "The  380 th epoch : Loss in Train:  4.655675888061523\n",
      "The  400 th epoch : Loss in Train:  4.656033515930176\n",
      "The  420 th epoch : Loss in Train:  4.636067867279053\n",
      "The  440 th epoch : Loss in Train:  4.6185784339904785\n",
      "The  460 th epoch : Loss in Train:  4.578488349914551\n",
      "The  480 th epoch : Loss in Train:  4.56447696685791\n",
      "The  500 th epoch : Loss in Train:  4.542949199676514\n",
      "The  520 th epoch : Loss in Train:  4.48489236831665\n",
      "The  540 th epoch : Loss in Train:  4.425703048706055\n",
      "The  560 th epoch : Loss in Train:  4.420191287994385\n",
      "The  580 th epoch : Loss in Train:  4.403562068939209\n",
      "The  600 th epoch : Loss in Train:  4.421994686126709\n",
      "The  620 th epoch : Loss in Train:  4.402345180511475\n",
      "The  640 th epoch : Loss in Train:  4.386405944824219\n",
      "The  660 th epoch : Loss in Train:  4.414721965789795\n",
      "The  680 th epoch : Loss in Train:  4.367258548736572\n",
      "The  700 th epoch : Loss in Train:  4.369129657745361\n",
      "The  720 th epoch : Loss in Train:  4.407754421234131\n",
      "The  740 th epoch : Loss in Train:  4.345052242279053\n",
      "The  760 th epoch : Loss in Train:  4.341264724731445\n",
      "The  780 th epoch : Loss in Train:  4.348243713378906\n",
      "The  800 th epoch : Loss in Train:  4.329158782958984\n",
      "The  820 th epoch : Loss in Train:  4.364312171936035\n",
      "The  840 th epoch : Loss in Train:  4.312765598297119\n",
      "The  860 th epoch : Loss in Train:  4.308155059814453\n",
      "The  880 th epoch : Loss in Train:  4.294164180755615\n",
      "The  900 th epoch : Loss in Train:  4.312985420227051\n",
      "The  920 th epoch : Loss in Train:  4.294752597808838\n",
      "The  940 th epoch : Loss in Train:  4.316347122192383\n",
      "The  960 th epoch : Loss in Train:  4.292311191558838\n",
      "The  980 th epoch : Loss in Train:  4.292074680328369\n",
      "The  1000 th epoch : Loss in Train:  4.3084635734558105\n",
      "The  1020 th epoch : Loss in Train:  4.252378940582275\n",
      "The  1040 th epoch : Loss in Train:  4.272048473358154\n",
      "The  1060 th epoch : Loss in Train:  4.339318752288818\n",
      "The  1080 th epoch : Loss in Train:  4.273155689239502\n",
      "The  1100 th epoch : Loss in Train:  4.227025032043457\n",
      "The  1120 th epoch : Loss in Train:  4.232141971588135\n",
      "The  1140 th epoch : Loss in Train:  4.217414379119873\n",
      "The  1160 th epoch : Loss in Train:  4.255112171173096\n",
      "The  1180 th epoch : Loss in Train:  4.262444496154785\n",
      "The  1200 th epoch : Loss in Train:  4.1883392333984375\n",
      "The  1220 th epoch : Loss in Train:  4.249209403991699\n",
      "The  1240 th epoch : Loss in Train:  4.289684772491455\n",
      "The  1260 th epoch : Loss in Train:  4.237353801727295\n",
      "The  1280 th epoch : Loss in Train:  4.184217929840088\n",
      "The  1300 th epoch : Loss in Train:  4.25358772277832\n",
      "The  1320 th epoch : Loss in Train:  4.183063507080078\n",
      "The  1340 th epoch : Loss in Train:  4.3420939445495605\n",
      "The  1360 th epoch : Loss in Train:  4.274020195007324\n",
      "The  1380 th epoch : Loss in Train:  4.204626560211182\n",
      "The  1400 th epoch : Loss in Train:  4.157125949859619\n",
      "The  1420 th epoch : Loss in Train:  4.347097396850586\n",
      "The  1440 th epoch : Loss in Train:  4.169948577880859\n",
      "The  1460 th epoch : Loss in Train:  4.233514308929443\n",
      "The  1480 th epoch : Loss in Train:  4.217604160308838\n",
      "The  1500 th epoch : Loss in Train:  4.141870021820068\n",
      "The  1520 th epoch : Loss in Train:  4.252714157104492\n",
      "The  1540 th epoch : Loss in Train:  4.278131008148193\n",
      "The  1560 th epoch : Loss in Train:  4.149636268615723\n",
      "The  1580 th epoch : Loss in Train:  4.166720390319824\n",
      "The  1600 th epoch : Loss in Train:  4.297415256500244\n",
      "The  1620 th epoch : Loss in Train:  4.174508571624756\n",
      "The  1640 th epoch : Loss in Train:  4.19452428817749\n",
      "The  1660 th epoch : Loss in Train:  4.37097692489624\n",
      "The  1680 th epoch : Loss in Train:  4.122582912445068\n",
      "The  1700 th epoch : Loss in Train:  4.178206443786621\n",
      "The  1720 th epoch : Loss in Train:  4.18827486038208\n",
      "The  1740 th epoch : Loss in Train:  4.1728692054748535\n",
      "The  1760 th epoch : Loss in Train:  4.234567642211914\n",
      "The  1780 th epoch : Loss in Train:  4.166413307189941\n",
      "The  1800 th epoch : Loss in Train:  4.174968242645264\n",
      "The  1820 th epoch : Loss in Train:  4.42239236831665\n",
      "The  1840 th epoch : Loss in Train:  4.245894908905029\n",
      "The  1860 th epoch : Loss in Train:  4.216741561889648\n",
      "The  1880 th epoch : Loss in Train:  4.140609264373779\n",
      "The  1900 th epoch : Loss in Train:  4.114976406097412\n",
      "The  1920 th epoch : Loss in Train:  4.120141506195068\n",
      "The  1940 th epoch : Loss in Train:  4.16141939163208\n",
      "The  1960 th epoch : Loss in Train:  4.186028003692627\n",
      "The  1980 th epoch : Loss in Train:  4.17950439453125\n",
      "The  2000 th epoch : Loss in Train:  4.179084300994873\n",
      "The  2020 th epoch : Loss in Train:  4.130322456359863\n",
      "The  2040 th epoch : Loss in Train:  4.122664928436279\n",
      "The  2060 th epoch : Loss in Train:  4.109194278717041\n",
      "The  2080 th epoch : Loss in Train:  4.182610034942627\n",
      "The  2100 th epoch : Loss in Train:  4.168041706085205\n",
      "The  2120 th epoch : Loss in Train:  4.1179704666137695\n",
      "The  2140 th epoch : Loss in Train:  4.1858439445495605\n",
      "The  2160 th epoch : Loss in Train:  4.187305927276611\n",
      "The  2180 th epoch : Loss in Train:  4.139830112457275\n",
      "The  2200 th epoch : Loss in Train:  4.140292167663574\n",
      "The  2220 th epoch : Loss in Train:  4.064486980438232\n",
      "The  2240 th epoch : Loss in Train:  4.245807647705078\n",
      "The  2260 th epoch : Loss in Train:  4.442625522613525\n",
      "The  2280 th epoch : Loss in Train:  4.270666122436523\n",
      "The  2300 th epoch : Loss in Train:  4.197073459625244\n",
      "The  2320 th epoch : Loss in Train:  4.0929789543151855\n",
      "The  2340 th epoch : Loss in Train:  4.265265464782715\n",
      "The  2360 th epoch : Loss in Train:  4.201745510101318\n",
      "The  2380 th epoch : Loss in Train:  4.223352909088135\n",
      "The  2400 th epoch : Loss in Train:  4.207950592041016\n",
      "The  2420 th epoch : Loss in Train:  4.071479320526123\n",
      "The  2440 th epoch : Loss in Train:  4.171625137329102\n",
      "The  2460 th epoch : Loss in Train:  4.177132606506348\n",
      "The  2480 th epoch : Loss in Train:  4.074481964111328\n",
      "The  2500 th epoch : Loss in Train:  4.40897798538208\n",
      "The  2520 th epoch : Loss in Train:  4.179208278656006\n",
      "The  2540 th epoch : Loss in Train:  4.179261684417725\n",
      "The  2560 th epoch : Loss in Train:  4.116003513336182\n",
      "The  2580 th epoch : Loss in Train:  4.1676459312438965\n",
      "The  2600 th epoch : Loss in Train:  4.226978302001953\n",
      "The  2620 th epoch : Loss in Train:  4.18112325668335\n",
      "The  2640 th epoch : Loss in Train:  4.180854320526123\n",
      "The  2660 th epoch : Loss in Train:  4.116481304168701\n",
      "The  2680 th epoch : Loss in Train:  4.079451084136963\n",
      "The  2700 th epoch : Loss in Train:  4.369254112243652\n",
      "The  2720 th epoch : Loss in Train:  4.12255334854126\n",
      "The  2740 th epoch : Loss in Train:  4.257876873016357\n",
      "The  2760 th epoch : Loss in Train:  4.048570156097412\n",
      "The  2780 th epoch : Loss in Train:  4.233689308166504\n",
      "The  2800 th epoch : Loss in Train:  4.12613582611084\n",
      "The  2820 th epoch : Loss in Train:  4.113516330718994\n",
      "The  2840 th epoch : Loss in Train:  4.15454626083374\n",
      "The  2860 th epoch : Loss in Train:  4.311774253845215\n",
      "The  2880 th epoch : Loss in Train:  4.233776569366455\n",
      "The  2900 th epoch : Loss in Train:  4.34637451171875\n",
      "The  2920 th epoch : Loss in Train:  4.075504779815674\n",
      "The  2940 th epoch : Loss in Train:  4.298684597015381\n",
      "The  2960 th epoch : Loss in Train:  4.1365509033203125\n",
      "The  2980 th epoch : Loss in Train:  4.290694713592529\n",
      "The  3000 th epoch : Loss in Train:  4.092171669006348\n",
      "The  3020 th epoch : Loss in Train:  4.199418067932129\n",
      "The  3040 th epoch : Loss in Train:  4.404712200164795\n",
      "The  3060 th epoch : Loss in Train:  4.244392395019531\n",
      "The  3080 th epoch : Loss in Train:  4.221235752105713\n",
      "The  3100 th epoch : Loss in Train:  4.091603755950928\n",
      "The  3120 th epoch : Loss in Train:  4.060995578765869\n",
      "The  3140 th epoch : Loss in Train:  4.196045875549316\n",
      "The  3160 th epoch : Loss in Train:  4.112992763519287\n",
      "The  3180 th epoch : Loss in Train:  4.212154388427734\n",
      "The  3200 th epoch : Loss in Train:  4.182651519775391\n",
      "The  3220 th epoch : Loss in Train:  4.284756183624268\n",
      "The  3240 th epoch : Loss in Train:  4.051687717437744\n",
      "The  3260 th epoch : Loss in Train:  4.034763813018799\n",
      "The  3280 th epoch : Loss in Train:  4.074671268463135\n",
      "The  3300 th epoch : Loss in Train:  4.206582546234131\n",
      "The  3320 th epoch : Loss in Train:  4.354122161865234\n",
      "The  3340 th epoch : Loss in Train:  4.053776741027832\n",
      "The  3360 th epoch : Loss in Train:  4.199906826019287\n",
      "The  3380 th epoch : Loss in Train:  4.054321765899658\n",
      "The  3400 th epoch : Loss in Train:  4.118117332458496\n",
      "The  3420 th epoch : Loss in Train:  4.152061462402344\n",
      "The  3440 th epoch : Loss in Train:  4.343692302703857\n",
      "The  3460 th epoch : Loss in Train:  4.098574161529541\n",
      "The  3480 th epoch : Loss in Train:  4.020737648010254\n",
      "The  3500 th epoch : Loss in Train:  4.160120964050293\n",
      "The  3520 th epoch : Loss in Train:  4.328832626342773\n",
      "The  3540 th epoch : Loss in Train:  4.302585601806641\n",
      "The  3560 th epoch : Loss in Train:  4.100778579711914\n",
      "The  3580 th epoch : Loss in Train:  4.082404136657715\n",
      "The  3600 th epoch : Loss in Train:  4.355020523071289\n",
      "The  3620 th epoch : Loss in Train:  4.112093448638916\n",
      "The  3640 th epoch : Loss in Train:  4.04857873916626\n",
      "The  3660 th epoch : Loss in Train:  4.184634685516357\n",
      "The  3680 th epoch : Loss in Train:  4.11942195892334\n",
      "The  3700 th epoch : Loss in Train:  4.052423000335693\n",
      "The  3720 th epoch : Loss in Train:  4.374709129333496\n",
      "The  3740 th epoch : Loss in Train:  4.253484725952148\n",
      "The  3760 th epoch : Loss in Train:  4.268985748291016\n",
      "The  3780 th epoch : Loss in Train:  4.1002092361450195\n",
      "The  3800 th epoch : Loss in Train:  4.188656806945801\n",
      "The  3820 th epoch : Loss in Train:  4.16550874710083\n",
      "The  3840 th epoch : Loss in Train:  4.042389869689941\n",
      "The  3860 th epoch : Loss in Train:  4.175205230712891\n",
      "The  3880 th epoch : Loss in Train:  4.105273723602295\n",
      "The  3900 th epoch : Loss in Train:  4.134400844573975\n",
      "The  3920 th epoch : Loss in Train:  4.131828308105469\n",
      "The  3940 th epoch : Loss in Train:  4.0262532234191895\n",
      "The  3960 th epoch : Loss in Train:  4.126275539398193\n",
      "The  3980 th epoch : Loss in Train:  4.044308662414551\n",
      "The  4000 th epoch : Loss in Train:  4.0908989906311035\n",
      "The  4020 th epoch : Loss in Train:  4.061540126800537\n",
      "The  4040 th epoch : Loss in Train:  4.014723300933838\n",
      "The  4060 th epoch : Loss in Train:  4.268165588378906\n",
      "The  4080 th epoch : Loss in Train:  4.313000202178955\n",
      "The  4100 th epoch : Loss in Train:  4.072460174560547\n",
      "The  4120 th epoch : Loss in Train:  4.120639324188232\n",
      "The  4140 th epoch : Loss in Train:  4.0005693435668945\n",
      "The  4160 th epoch : Loss in Train:  4.338597297668457\n",
      "The  4180 th epoch : Loss in Train:  4.178346157073975\n",
      "The  4200 th epoch : Loss in Train:  4.024643421173096\n",
      "The  4220 th epoch : Loss in Train:  3.9853315353393555\n",
      "The  4240 th epoch : Loss in Train:  4.167201995849609\n",
      "The  4260 th epoch : Loss in Train:  4.152583122253418\n",
      "The  4280 th epoch : Loss in Train:  4.134586811065674\n",
      "The  4300 th epoch : Loss in Train:  4.2023162841796875\n",
      "The  4320 th epoch : Loss in Train:  4.004021167755127\n",
      "The  4340 th epoch : Loss in Train:  4.187709808349609\n",
      "The  4360 th epoch : Loss in Train:  4.326864719390869\n",
      "The  4380 th epoch : Loss in Train:  4.293910503387451\n",
      "The  4400 th epoch : Loss in Train:  4.230157375335693\n",
      "The  4420 th epoch : Loss in Train:  4.072559356689453\n",
      "The  4440 th epoch : Loss in Train:  4.074954509735107\n",
      "The  4460 th epoch : Loss in Train:  4.048086643218994\n",
      "The  4480 th epoch : Loss in Train:  4.185573577880859\n",
      "The  4500 th epoch : Loss in Train:  4.257084369659424\n",
      "The  4520 th epoch : Loss in Train:  4.095881462097168\n",
      "The  4540 th epoch : Loss in Train:  4.070777416229248\n",
      "The  4560 th epoch : Loss in Train:  4.0072479248046875\n",
      "The  4580 th epoch : Loss in Train:  4.123437404632568\n",
      "The  4600 th epoch : Loss in Train:  4.052881240844727\n",
      "The  4620 th epoch : Loss in Train:  4.1319499015808105\n",
      "The  4640 th epoch : Loss in Train:  4.034253120422363\n",
      "The  4660 th epoch : Loss in Train:  4.087313175201416\n",
      "The  4680 th epoch : Loss in Train:  4.156432628631592\n",
      "The  4700 th epoch : Loss in Train:  4.085957050323486\n",
      "The  4720 th epoch : Loss in Train:  4.063955783843994\n",
      "The  4740 th epoch : Loss in Train:  4.083687782287598\n",
      "The  4760 th epoch : Loss in Train:  4.026210784912109\n",
      "The  4780 th epoch : Loss in Train:  4.011962413787842\n",
      "The  4800 th epoch : Loss in Train:  4.082295894622803\n",
      "The  4820 th epoch : Loss in Train:  4.183021545410156\n",
      "The  4840 th epoch : Loss in Train:  3.9678361415863037\n",
      "The  4860 th epoch : Loss in Train:  4.015085220336914\n",
      "The  4880 th epoch : Loss in Train:  4.121229648590088\n",
      "The  4900 th epoch : Loss in Train:  4.040549278259277\n",
      "The  4920 th epoch : Loss in Train:  4.09018087387085\n",
      "The  4940 th epoch : Loss in Train:  3.932396411895752\n",
      "The  4960 th epoch : Loss in Train:  4.213037014007568\n",
      "The  4980 th epoch : Loss in Train:  3.954512357711792\n",
      "The  5000 th epoch : Loss in Train:  3.967454671859741\n",
      "the lamda is:  0.01 , the learning rate is:  0.0001 .\n",
      "the Loss in Train data is: 3.967454671859741  Loss in Validation is:  3.1889264583587646\n",
      "there are  5000 epochs in the training process!!\n",
      "The  0 th epoch : Loss in Train:  4.699743270874023\n",
      "The  20 th epoch : Loss in Train:  4.698784828186035\n",
      "The  40 th epoch : Loss in Train:  4.697653770446777\n",
      "The  60 th epoch : Loss in Train:  4.696522235870361\n",
      "The  80 th epoch : Loss in Train:  4.695167064666748\n",
      "The  100 th epoch : Loss in Train:  4.6941752433776855\n",
      "The  120 th epoch : Loss in Train:  4.692722320556641\n",
      "The  140 th epoch : Loss in Train:  4.691891670227051\n",
      "The  160 th epoch : Loss in Train:  4.6904215812683105\n",
      "The  180 th epoch : Loss in Train:  4.688919544219971\n",
      "The  200 th epoch : Loss in Train:  4.688461780548096\n",
      "The  220 th epoch : Loss in Train:  4.68677282333374\n",
      "The  240 th epoch : Loss in Train:  4.685117244720459\n",
      "The  260 th epoch : Loss in Train:  4.683888912200928\n",
      "The  280 th epoch : Loss in Train:  4.682450771331787\n",
      "The  300 th epoch : Loss in Train:  4.680421352386475\n",
      "The  320 th epoch : Loss in Train:  4.679480075836182\n",
      "The  340 th epoch : Loss in Train:  4.677056789398193\n",
      "The  360 th epoch : Loss in Train:  4.674572944641113\n",
      "The  380 th epoch : Loss in Train:  4.671957492828369\n",
      "The  400 th epoch : Loss in Train:  4.669453144073486\n",
      "The  420 th epoch : Loss in Train:  4.665182590484619\n",
      "The  440 th epoch : Loss in Train:  4.662554740905762\n",
      "The  460 th epoch : Loss in Train:  4.64552640914917\n",
      "The  480 th epoch : Loss in Train:  4.624240875244141\n",
      "The  500 th epoch : Loss in Train:  4.62369966506958\n",
      "The  520 th epoch : Loss in Train:  4.620729923248291\n",
      "The  540 th epoch : Loss in Train:  4.580395698547363\n",
      "The  560 th epoch : Loss in Train:  4.546292304992676\n",
      "The  580 th epoch : Loss in Train:  4.530426025390625\n",
      "The  600 th epoch : Loss in Train:  4.534383773803711\n",
      "The  620 th epoch : Loss in Train:  4.497628688812256\n",
      "The  640 th epoch : Loss in Train:  4.467395782470703\n",
      "The  660 th epoch : Loss in Train:  4.421683311462402\n",
      "The  680 th epoch : Loss in Train:  4.4124908447265625\n",
      "The  700 th epoch : Loss in Train:  4.410746097564697\n",
      "The  720 th epoch : Loss in Train:  4.442486763000488\n",
      "The  740 th epoch : Loss in Train:  4.411869525909424\n",
      "The  760 th epoch : Loss in Train:  4.432438373565674\n",
      "The  780 th epoch : Loss in Train:  4.406332492828369\n",
      "The  800 th epoch : Loss in Train:  4.4609599113464355\n",
      "The  820 th epoch : Loss in Train:  4.417864799499512\n",
      "The  840 th epoch : Loss in Train:  4.3825554847717285\n",
      "The  860 th epoch : Loss in Train:  4.366533279418945\n",
      "The  880 th epoch : Loss in Train:  4.36857271194458\n",
      "The  900 th epoch : Loss in Train:  4.355517387390137\n",
      "The  920 th epoch : Loss in Train:  4.4230828285217285\n",
      "The  940 th epoch : Loss in Train:  4.355123996734619\n",
      "The  960 th epoch : Loss in Train:  4.3320159912109375\n",
      "The  980 th epoch : Loss in Train:  4.348438262939453\n",
      "The  1000 th epoch : Loss in Train:  4.41086483001709\n",
      "The  1020 th epoch : Loss in Train:  4.3424811363220215\n",
      "The  1040 th epoch : Loss in Train:  4.379584789276123\n",
      "The  1060 th epoch : Loss in Train:  4.329441547393799\n",
      "The  1080 th epoch : Loss in Train:  4.3249969482421875\n",
      "The  1100 th epoch : Loss in Train:  4.310685634613037\n",
      "The  1120 th epoch : Loss in Train:  4.30300760269165\n",
      "The  1140 th epoch : Loss in Train:  4.3643293380737305\n",
      "The  1160 th epoch : Loss in Train:  4.34441614151001\n",
      "The  1180 th epoch : Loss in Train:  4.337131500244141\n",
      "The  1200 th epoch : Loss in Train:  4.318891525268555\n",
      "The  1220 th epoch : Loss in Train:  4.33514404296875\n",
      "The  1240 th epoch : Loss in Train:  4.300879001617432\n",
      "The  1260 th epoch : Loss in Train:  4.303715705871582\n",
      "The  1280 th epoch : Loss in Train:  4.294947624206543\n",
      "The  1300 th epoch : Loss in Train:  4.303431034088135\n",
      "The  1320 th epoch : Loss in Train:  4.440255165100098\n",
      "The  1340 th epoch : Loss in Train:  4.289141654968262\n",
      "The  1360 th epoch : Loss in Train:  4.316618919372559\n",
      "The  1380 th epoch : Loss in Train:  4.290219306945801\n",
      "The  1400 th epoch : Loss in Train:  4.278197765350342\n",
      "The  1420 th epoch : Loss in Train:  4.269167423248291\n",
      "The  1440 th epoch : Loss in Train:  4.257991790771484\n",
      "The  1460 th epoch : Loss in Train:  4.326623916625977\n",
      "The  1480 th epoch : Loss in Train:  4.280787944793701\n",
      "The  1500 th epoch : Loss in Train:  4.3007097244262695\n",
      "The  1520 th epoch : Loss in Train:  4.3661394119262695\n",
      "The  1540 th epoch : Loss in Train:  4.37990140914917\n",
      "The  1560 th epoch : Loss in Train:  4.3335862159729\n",
      "The  1580 th epoch : Loss in Train:  4.263497829437256\n",
      "The  1600 th epoch : Loss in Train:  4.227106094360352\n",
      "The  1620 th epoch : Loss in Train:  4.304306983947754\n",
      "The  1640 th epoch : Loss in Train:  4.255182266235352\n",
      "The  1660 th epoch : Loss in Train:  4.199897766113281\n",
      "The  1680 th epoch : Loss in Train:  4.271635055541992\n",
      "The  1700 th epoch : Loss in Train:  4.2452006340026855\n",
      "The  1720 th epoch : Loss in Train:  4.282881259918213\n",
      "The  1740 th epoch : Loss in Train:  4.277650833129883\n",
      "The  1760 th epoch : Loss in Train:  4.275020599365234\n",
      "The  1780 th epoch : Loss in Train:  4.19503927230835\n",
      "The  1800 th epoch : Loss in Train:  4.220190525054932\n",
      "The  1820 th epoch : Loss in Train:  4.2057976722717285\n",
      "The  1840 th epoch : Loss in Train:  4.204252243041992\n",
      "The  1860 th epoch : Loss in Train:  4.228443145751953\n",
      "The  1880 th epoch : Loss in Train:  4.358736991882324\n",
      "The  1900 th epoch : Loss in Train:  4.2205810546875\n",
      "The  1920 th epoch : Loss in Train:  4.18540096282959\n",
      "The  1940 th epoch : Loss in Train:  4.237606525421143\n",
      "The  1960 th epoch : Loss in Train:  4.1806640625\n",
      "The  1980 th epoch : Loss in Train:  4.285457611083984\n",
      "The  2000 th epoch : Loss in Train:  4.51989221572876\n",
      "The  2020 th epoch : Loss in Train:  4.227741241455078\n",
      "The  2040 th epoch : Loss in Train:  4.30866813659668\n",
      "The  2060 th epoch : Loss in Train:  4.186933994293213\n",
      "The  2080 th epoch : Loss in Train:  4.274387359619141\n",
      "The  2100 th epoch : Loss in Train:  4.224207878112793\n",
      "The  2120 th epoch : Loss in Train:  4.286381721496582\n",
      "The  2140 th epoch : Loss in Train:  4.165971755981445\n",
      "The  2160 th epoch : Loss in Train:  4.19071102142334\n",
      "The  2180 th epoch : Loss in Train:  4.188765525817871\n",
      "The  2200 th epoch : Loss in Train:  4.382236003875732\n",
      "The  2220 th epoch : Loss in Train:  4.206570625305176\n",
      "The  2240 th epoch : Loss in Train:  4.273427486419678\n",
      "The  2260 th epoch : Loss in Train:  4.266468524932861\n",
      "The  2280 th epoch : Loss in Train:  4.302488803863525\n",
      "The  2300 th epoch : Loss in Train:  4.2068915367126465\n",
      "The  2320 th epoch : Loss in Train:  4.198734283447266\n",
      "The  2340 th epoch : Loss in Train:  4.216763019561768\n",
      "The  2360 th epoch : Loss in Train:  4.2178239822387695\n",
      "The  2380 th epoch : Loss in Train:  4.1769633293151855\n",
      "The  2400 th epoch : Loss in Train:  4.149916172027588\n",
      "The  2420 th epoch : Loss in Train:  4.326291561126709\n",
      "The  2440 th epoch : Loss in Train:  4.215826034545898\n",
      "The  2460 th epoch : Loss in Train:  4.224726676940918\n",
      "The  2480 th epoch : Loss in Train:  4.21188497543335\n",
      "The  2500 th epoch : Loss in Train:  4.321653842926025\n",
      "The  2520 th epoch : Loss in Train:  4.257096290588379\n",
      "The  2540 th epoch : Loss in Train:  4.288145065307617\n",
      "The  2560 th epoch : Loss in Train:  4.2751784324646\n",
      "The  2580 th epoch : Loss in Train:  4.155122756958008\n",
      "The  2600 th epoch : Loss in Train:  4.150157928466797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l2 \u001b[38;5;129;01min\u001b[39;00m L2_Lambda:\n\u001b[0;32m     68\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m Initial_Learning_Rate:\n\u001b[1;32m---> 69\u001b[0m \t\tloss_train, loss_valid, c_index_tr, c_index_va, net \u001b[38;5;241m=\u001b[39m trainDeepOmixNet_without(x_train, ytime_train, yevent_train, \\\n\u001b[0;32m     70\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx_valid,  ytime_valid, yevent_valid, pathway_mask, \\\n\u001b[0;32m     71\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n\u001b[0;32m     72\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr, l2, num_epochs, Dropout_Rate)\n\u001b[0;32m     74\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m loss_valid \u001b[38;5;241m<\u001b[39m opt_loss:\n\u001b[0;32m     75\u001b[0m \t\t\topt_l2_loss \u001b[38;5;241m=\u001b[39m l2\n",
      "Cell \u001b[1;32mIn[8], line 117\u001b[0m, in \u001b[0;36mtrainDeepOmixNet_without\u001b[1;34m(train_x, train_ytime, train_yevent, eval_x, eval_ytime, eval_yevent, pathway_mask, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, Learning_Rate, L2, Num_Epochs, Dropout_Rate)\u001b[0m\n\u001b[0;32m    114\u001b[0m eval_pred \u001b[38;5;241m=\u001b[39m net(eval_x, eval_yevent)\n\u001b[0;32m    115\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 117\u001b[0m train_cindex \u001b[38;5;241m=\u001b[39m c_index(train_pred, train_ytime, train_yevent)\n\u001b[0;32m    118\u001b[0m eval_cindex \u001b[38;5;241m=\u001b[39m c_index(eval_pred, eval_ytime, eval_yevent)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth epoch : Loss in Train: \u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss)\n",
      "File \u001b[1;32mD:\\Desktop\\bioinfo-company\\-\\19.wujiaoxiang.fss.biyanai\\review-20240311\\DeepOmix-main\\Survival.py:32\u001b[0m, in \u001b[0;36mc_index\u001b[1;34m(pred, ytime, yevent)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_sample):\n\u001b[0;32m     31\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m pred[i] \u001b[38;5;241m<\u001b[39m pred[j]:\n\u001b[1;32m---> 32\u001b[0m \t\tpred_matrix[j, i]  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m \t\u001b[38;5;28;01melif\u001b[39;00m pred[i] \u001b[38;5;241m==\u001b[39m pred[j]: \n\u001b[0;32m     34\u001b[0m \t\tpred_matrix[j, i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prepare model and train\n",
    "# prepare model\n",
    "from DataLoader import load_data_without, load_pathway\n",
    "# from Train import trainDeepOmixNet_without\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "gpu_id=0\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "\ttorch.cuda.set_device(gpu_id)\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "In_Nodes = 39\n",
    "Pathway_Nodes = 100 \n",
    "Hidden_Nodes = 80\n",
    "Out_Nodes = 30\n",
    "\n",
    "Initial_Learning_Rate = [0.003, 0.001, 0.0001, 0.000075]\n",
    "L2_Lambda = [0.1, 0.01, 0.005, 0.001]\n",
    "num_epochs = 5000\n",
    "Num_EPOCHS = 20000\n",
    "Dropout_Rate = [0.7, 0.5]\n",
    "\n",
    "path='./Data/Multiple/'\n",
    "pathway_mask = load_pathway(path+\"pathway_module_input.csv\", dtype)\n",
    "\n",
    "pathway_mask = torch.ones(Pathway_Nodes, 39)\n",
    "\n",
    "opt_l2_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "opt_c_index_va = 0\n",
    "opt_c_index_tr = 0\n",
    "\n",
    "# x_train = torch.from_numpy(X_train.iloc[1:300,:].values).type(dtype).to(device)\n",
    "# ytime_train = torch.from_numpy(y_time_train.iloc[1:300,:].values).type(dtype).to(device)\n",
    "# yevent_train = torch.from_numpy(y_event_train.iloc[1:300,:].values).type(dtype).to(device)\n",
    "\n",
    "# x_valid = torch.from_numpy(X_train.iloc[300:,:].values).type(dtype).to(device)\n",
    "# ytime_valid = torch.from_numpy(y_time_train.iloc[300:,:].values).type(dtype).to(device)\n",
    "# yevent_valid = torch.from_numpy(y_event_train.iloc[300:,:].values).type(dtype).to(device)\n",
    "\n",
    "x_train = torch.from_numpy(X_train.iloc[1:300,:].values).type(dtype)\n",
    "ytime_train = torch.from_numpy(y_time_train.iloc[1:300,:].values).type(dtype)\n",
    "yevent_train = torch.from_numpy(y_event_train.iloc[1:300,:].values).type(dtype)\n",
    "\n",
    "x_valid = torch.from_numpy(X_train.iloc[300:,:].values).type(dtype)\n",
    "ytime_valid = torch.from_numpy(y_time_train.iloc[300:,:].values).type(dtype)\n",
    "yevent_valid = torch.from_numpy(y_event_train.iloc[300:,:].values).type(dtype)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\topt_loss = opt_loss.cuda()\n",
    "\tyevent_valid = yevent_valid.cuda()\n",
    "\tytime_valid = ytime_valid.cuda()\n",
    "\tx_valid = x_valid.cuda()\n",
    "\tyevent_train = yevent_train.cuda()\n",
    "\tpathway_mask = pathway_mask.cuda()\n",
    "\tytime_train = ytime_train.cuda()\n",
    "\tx_train = x_train.cuda()\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "for l2 in L2_Lambda:\n",
    "\tfor lr in Initial_Learning_Rate:\n",
    "\t\tloss_train, loss_valid, c_index_tr, c_index_va, net = trainDeepOmixNet_without(x_train, ytime_train, yevent_train, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx_valid,  ytime_valid, yevent_valid, pathway_mask, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr, l2, num_epochs, Dropout_Rate)\n",
    "\t\t\n",
    "\t\tif loss_valid < opt_loss:\n",
    "\t\t\topt_l2_loss = l2\n",
    "\t\t\topt_lr_loss = lr\n",
    "\t\t\topt_loss = loss_valid\n",
    "\t\t\topt_c_index_tr = c_index_tr\n",
    "\t\t\topt_c_index_va = c_index_va\n",
    "\t\t\topt_net = net\n",
    "\t\tprint (\"the lamda is: \", l2, \", the learning rate is: \", lr,\".\")\n",
    "\t\tprint (\"the Loss in Train data is:\",loss_train, \" Loss in Validation is: \", loss_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb1443d-51f6-47a6-9a1c-4df4cf1c9f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8057055473327637\n",
      "0.003\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(opt_loss)\n",
    "print(opt_lr_loss)\n",
    "print(opt_l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9e987f-aa44-47fa-b67d-beb21a17e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "## \n",
    "torch.save(opt_net, './ana_20240318/opt.net.model.in.ipynb.pth')\n",
    "\n",
    "## \n",
    "# model = torch.load('model_name.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e93722a-07ee-4dc9-bdbe-165071cc5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8057055473327637\n",
      "0.003\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# first round run\n",
    "print(opt_loss)\n",
    "print(opt_lr_loss)\n",
    "print(opt_l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a3eaeb5-f076-4969-a70f-2d5cddc8829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "## \n",
    "torch.save(net, './ana_20240318/lr.0.003.l2.0.1.model.pth')\n",
    "\n",
    "## \n",
    "# model = torch.load('model_name.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a12c913-0cea-42a1-9d27-d337c2601397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test dataset\n",
    "X_test = test_df.loc[:,x_columns]\n",
    "y_time_test = test_df.loc[:,['OS']]\n",
    "y_event_test = test_df.loc[:,['survival']]\n",
    "\n",
    "x_test = torch.from_numpy(X_test.iloc[1:300,:].values).type(dtype)\n",
    "ytime_test = torch.from_numpy(y_time_test.iloc[1:300,:].values).type(dtype)\n",
    "yevent_test = torch.from_numpy(y_event_test.iloc[1:300,:].values).type(dtype)\n",
    "\n",
    "x_test = x_test.cuda()\n",
    "ytime_test = ytime_test.cuda()\n",
    "yevent_test = yevent_test.cuda()\n",
    "\n",
    "ytest_pred = net(x_test, yevent_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "931a9d8a-0d66-46bc-8a90-46aab9941cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(ytest_pred.cpu().detach())\n",
    "tmp_df.columns = ['ytest_prediction']\n",
    "tmp_df['ytest_time'] = ytime_test.cpu().detach()\n",
    "tmp_df['ytest_event'] = yevent_test.cpu().detach()\n",
    "tmp_df\n",
    "tmp_df.to_csv('./ana_20240318/lr.0.003.l2.0.1.model.prediction.test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9de0316-5ffc-4c78-adca-5bc3b0bbfcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train data\n",
    "x_train1 = torch.from_numpy(X_train.values).type(dtype)\n",
    "ytime_train1 = torch.from_numpy(y_time_train.values).type(dtype)\n",
    "yevent_train1 = torch.from_numpy(y_event_train.values).type(dtype)\n",
    "\n",
    "# x_valid = torch.from_numpy(X_train.iloc[300:,:].values).type(dtype)\n",
    "# ytime_valid = torch.from_numpy(y_time_train.iloc[300:,:].values).type(dtype)\n",
    "# yevent_valid = torch.from_numpy(y_event_train.iloc[300:,:].values).type(dtype)\n",
    "\n",
    "x_train1 = x_train1.cuda()\n",
    "ytime_train1 = ytime_train1.cuda()\n",
    "yevent_train1 = yevent_train1.cuda()\n",
    "\n",
    "model = torch.load('./ana_20240318/lr.0.003.l2.0.1.model.pth')\n",
    "\n",
    "ytrain_pred1 = model(x_train1, yevent_train1) \n",
    "# if torch.cuda.is_available():\n",
    "# \topt_loss = opt_loss.cuda()\n",
    "# \tyevent_valid = yevent_valid.cuda()\n",
    "# \tytime_valid = ytime_valid.cuda()\n",
    "# \tx_valid = x_valid.cuda()\n",
    "# \tyevent_train = yevent_train.cuda()\n",
    "# \tpathway_mask = pathway_mask.cuda()\n",
    "# \tytime_train = ytime_train.cuda()\n",
    "# \tx_train = x_train.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ee490c-516f-40fd-b21e-c643e33dc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(ytrain_pred1.cpu().detach())\n",
    "tmp_df.columns = ['ytrain_prediction']\n",
    "tmp_df['ytrain_time'] = ytime_train1.cpu().detach()\n",
    "tmp_df['ytrain_event'] = yevent_train1.cpu().detach()\n",
    "tmp_df\n",
    "tmp_df.to_csv('./ana_20240318/lr.0.003.l2.0.1.model.prediction.train.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d95220-fc09-4808-a1be-850f8bfb94fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ytrain_prediction</th>\n",
       "      <th>ytrain_time</th>\n",
       "      <th>ytrain_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.518758e-20</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.036124e-21</td>\n",
       "      <td>5255.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.622176e-20</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.753628e-20</td>\n",
       "      <td>455.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.781232e-20</td>\n",
       "      <td>384.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>-1.407417e-20</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-1.443076e-20</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-1.643067e-20</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2.257354e+00</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-1.103952e-20</td>\n",
       "      <td>3761.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ytrain_prediction  ytrain_time  ytrain_event\n",
       "0        -1.518758e-20        714.0           0.0\n",
       "1        -8.036124e-21       5255.0           0.0\n",
       "2        -1.622176e-20        482.0           0.0\n",
       "3        -1.753628e-20        455.0           0.0\n",
       "4        -1.781232e-20        384.0           0.0\n",
       "..                 ...          ...           ...\n",
       "343      -1.407417e-20       1250.0           0.0\n",
       "344      -1.443076e-20         82.0           0.0\n",
       "345      -1.643067e-20        585.0           0.0\n",
       "346       2.257354e+00        162.0           1.0\n",
       "347      -1.103952e-20       3761.0           0.0\n",
       "\n",
       "[348 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ytest_pred\n",
    "ytrain_pred1\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da96edeb-7c5a-4d87-a8eb-f588c969e2a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Num_Epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthere are \u001b[39m\u001b[38;5;124m'\u001b[39m,Num_Epochs,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs in the training process!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m net \u001b[38;5;241m=\u001b[39m DeepOmixNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Num_Epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\tprint('there are ',Num_Epochs,'epochs in the training process!!')\n",
    "\tnet = DeepOmixNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)\n",
    "\t###if gpu is being used\n",
    "\t# if torch.cuda.is_available():\n",
    "\t# \tnet.cuda()\n",
    "\t# opt = optim.Adam(net.parameters(), lr=Learning_Rate, weight_decay = L2)\n",
    "\t# for epoch in range(Num_Epochs+1):\n",
    "\t# \tnet.train()\n",
    "\t# \topt.zero_grad() \n",
    "\t# \tnet.do_m1 = dropout_mask(Pathway_Nodes, Dropout_Rate[0])\n",
    "\t# \tnet.do_m2 = dropout_mask(Hidden_Nodes, Dropout_Rate[1])\n",
    "\n",
    "\t# \tpred = net(train_x, train_yevent) \n",
    "\t# \tloss = neg_par_log_likelihood(pred, train_ytime, train_yevent) \n",
    "\t# \tloss.backward()\n",
    "\t# \topt.step()\n",
    "\n",
    "\t# \tnet.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask)\n",
    "\n",
    "\t# \tdo_m1_grad = copy.deepcopy(net.sc2.weight._grad.data)\n",
    "\t# \tdo_m2_grad = copy.deepcopy(net.sc3.weight._grad.data)\n",
    "\t# \tdo_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))\n",
    "\t# \tdo_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))\n",
    "\n",
    "\t# \tnet_sc2_weight = copy.deepcopy(net.sc2.weight.data)\n",
    "\t# \tnet_sc3_weight = copy.deepcopy(net.sc3.weight.data)\n",
    "\n",
    "\t# \tnet_state_dict = net.state_dict()\n",
    "\n",
    "\t# \tcopy_net = copy.deepcopy(net)\n",
    "\t# \tcopy_state_dict = copy_net.state_dict()\n",
    "\t# \tfor name, param in copy_state_dict.items():\n",
    "\n",
    "\t# \t\tif not \"weight\" in name:\n",
    "\t# \t\t\tcontinue\n",
    "\t# \t\tif \"sc1\" in name:\n",
    "\t# \t\t\tcontinue\n",
    "\t# \t\tif \"sc4\" in name:\n",
    "\t# \t\t\tbreak\n",
    "\t# \t\tif \"sc2\" in name:\n",
    "\t# \t\t\tactive_param = net_sc2_weight.mul(do_m1_grad_mask)\n",
    "\t# \t\tif \"sc3\" in name:\n",
    "\t# \t\t\tactive_param = net_sc3_weight.mul(do_m2_grad_mask)\n",
    "\t# \t\tnonzero_param_1d = active_param[active_param != 0]\n",
    "\t# \t\tif nonzero_param_1d.size(0) == 0: \n",
    "\t# \t\t\tbreak\n",
    "\t# \t\tcopy_param_1d = copy.deepcopy(nonzero_param_1d)\n",
    "\t# \t\tS_set =  torch.arange(100, -1, -1)[1:]\n",
    "\t# \t\tcopy_param = copy.deepcopy(active_param)\n",
    "\t# \t\tS_loss = []\n",
    "\t# \t\tfor S in S_set:\n",
    "\t# \t\t\tparam_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "\t# \t\t\ttransformed_param = copy_param.mul(param_mask)\n",
    "\t# \t\t\tcopy_state_dict[name].copy_(transformed_param)\n",
    "\t# \t\t\tcopy_net.train()\n",
    "\t# \t\t\ty_tmp = copy_net(train_x, train_yevent)\n",
    "\t# \t\t\tloss_tmp = neg_par_log_likelihood(y_tmp, train_ytime, train_yevent)\n",
    "\t# \t\t\tS_loss.append(loss_tmp)\n",
    "\t# \t\tinterp_S_loss = interp1d(S_set, S_loss, kind='cubic')\n",
    "\t# \t\tinterp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)\n",
    "\t# \t\tinterp_loss = interp_S_loss(interp_S_set)\n",
    "\t# \t\toptimal_S = interp_S_set[np.argmin(interp_loss)]\n",
    "\t# \t\toptimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "\t# \t\tif \"sc2\" in name:\n",
    "\t# \t\t\tfinal_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)\n",
    "\t# \t\t\toptimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)\n",
    "\t# \t\tif \"sc3\" in name:\n",
    "\t# \t\t\tfinal_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)\n",
    "\t# \t\t\toptimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)\n",
    "\t# \t\tcopy_state_dict[name].copy_(optimal_transformed_param)\n",
    "\t# \t\tnet_state_dict[name].copy_(optimal_transformed_param)\n",
    "\n",
    "\t# \tif epoch % 20 == 0: \n",
    "\t# \t\tnet.train()\n",
    "\t# \t\ttrain_pred = net(train_x, train_yevent)\n",
    "\t# \t\ttrain_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent).view(1,).item()\n",
    "\n",
    "\t# \t\tnet.eval()\n",
    "\t# \t\teval_pred = net(eval_x, eval_yevent)\n",
    "\t# \t\teval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent).view(1,).item()\n",
    "\n",
    "\t# \t\ttrain_cindex = c_index(train_pred, train_ytime, train_yevent)\n",
    "\t# \t\teval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)\n",
    "\t# \t\tprint(\"The \",epoch,\"th epoch : Loss in Train: \", train_loss)\n",
    "\t# \t\tif (math.isnan(train_loss)):\n",
    "\t# \t\t\tprint(epoch,train_loss,\"end_train\")\n",
    "\t# \t\t\tbreak\n",
    "\n",
    "\t# return (train_loss, eval_loss, train_cindex, eval_cindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf505be-4295-453d-9a16-9b3e752df839",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n",
    "# train_df.columns\n",
    "# train_df.loc[:,['OS_x', 'OS_y']]\n",
    "# y_event_train\n",
    "# torch.from_numpy(X_train.iloc[300:,:].values).type(dtype)\n",
    "x_train\n",
    "# torch.ones(100)\n",
    "# y_time_train.iloc[1:10].values\n",
    "y_time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46081634-d70f-4f40-89ec-950486f91bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sample_list = \n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9e70dbc1-31d8-45f8-8f07-fafc4432abab",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mlist\u001b[39m(merge_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     35\u001b[0m test_index_i\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mlist\u001b[39m(merge_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues)[test_index_i]\n\u001b[1;32m     37\u001b[0m X_train\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "deg_df.iloc[:,0]\n",
    "dmp_df.iloc[:,0]\n",
    "mut_df.iloc[:,0]\n",
    "# sum(mut_df.iloc[:,0] == dmp_df.iloc[:,0])\n",
    "# sum(mut_df.iloc[:,0] == deg_df.iloc[:,0])\n",
    "# sum(dmp_df.iloc[:,0] == deg_df.iloc[:,0])\n",
    "deg_df.columns\n",
    "deg_coef\n",
    "# mut_df\n",
    "dmp_df\n",
    "# deg_df.patient01\n",
    "# deg_coef.gene_name.values\n",
    "# deg_df\n",
    "# # mut_coef.index.values[:]\n",
    "# # # mut_df.columns\n",
    "# # mut_select_df\n",
    "# # list(mut_coef.index.values)\n",
    "# # mut_select_df\n",
    "# # deg_select_df\n",
    "# [i in deg_df.columns for i in [\"patient\", \"OS\", \"survival\"] + list(deg_coef.gene_name.values)]\n",
    "# deg_l\n",
    "# dmp_coef\n",
    "dmp_select_df\n",
    "# mut_select_df\n",
    "# mut_sample\n",
    "mut_index_1\n",
    "# mut_df.loc[mut_df,:]\n",
    "tmp1\n",
    "merge_df.index.values\n",
    "len(train_index)\n",
    "len(test_index)\n",
    "# train_index + test_index \n",
    "# test_index\n",
    "list(merge_df.index.values)\n",
    "test_index_i\n",
    "list(merge_df.index.values)[test_index_i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adc340-6008-41ab-aef0-0e17874edcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "from DataLoader import load_data_without, load_pathway\n",
    "from Train import trainDeepOmixNet_without\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "gpu_id=0\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "\ttorch.cuda.set_device(gpu_id)\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "In_Nodes = 2708\n",
    "Pathway_Nodes = 100 \n",
    "Hidden_Nodes = 80\n",
    "Out_Nodes = 30\n",
    "\n",
    "Initial_Learning_Rate = [0.003, 0.001, 0.0001, 0.000075]\n",
    "L2_Lambda = [0.1, 0.01, 0.005, 0.001]\n",
    "num_epochs = 5000\n",
    "Num_EPOCHS = 20000\n",
    "Dropout_Rate = [0.7, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25856b79-15d7-410a-b4c0-f59823b7ba27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-FG-7643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-DU-8158</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-DU-5854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-DU-8161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-HT-7469</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>TCGA-QH-A6CW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>TCGA-CS-4942</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>TCGA-FG-A4MY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>TCGA-HT-7602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>TCGA-RY-A847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1  2  3  4  5  6  7  8     9\n",
       "0    TCGA-FG-7643  0  0  0  0  0  0  0  0   611\n",
       "1    TCGA-DU-8158  1  1  0  0  0  0  1  0   155\n",
       "2    TCGA-DU-5854  0  0  0  0  0  1  0  0   257\n",
       "3    TCGA-DU-8161  1  1  0  0  0  0  0  0   722\n",
       "4    TCGA-HT-7469  1  1  0  0  0  0  0  0   351\n",
       "..            ... .. .. .. .. .. .. .. ..   ...\n",
       "493  TCGA-QH-A6CW  0  0  0  0  1  0  0  1   414\n",
       "494  TCGA-CS-4942  1  1  0  0  1  0  0  0  1335\n",
       "495  TCGA-FG-A4MY  0  0  0  0  1  0  0  0   721\n",
       "496  TCGA-HT-7602  0  0  0  0  1  0  0  0   908\n",
       "497  TCGA-RY-A847  0  0  0  0  1  1  0  0   933\n",
       "\n",
       "[498 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deg_df\n",
    "# dmp_df\n",
    "# mut_df\n",
    "# help(pd.read_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
